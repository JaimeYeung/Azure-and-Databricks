{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60bd4081",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failure while loading azureml_run_type_providers. Failed to load entrypoint automl = azureml.train.automl.run:AutoMLRun._from_run_dto with exception (cloudpickle 2.0.0 (/opt/anaconda3/lib/python3.8/site-packages), Requirement.parse('cloudpickle<2.0.0,>=1.1.0'), {'azureml-dataprep'}).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SDK Version: 1.35.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import azureml.core\n",
    "from azureml.core import Workspace, Experiment, Datastore\n",
    "from azureml.widgets import RunDetails\n",
    "\n",
    "from azureml.core import Dataset\n",
    "\n",
    "from azureml.pipeline.core import Pipeline, PipelineData, PipelineRun, StepRun, PortDataReference\n",
    "from azureml.pipeline.steps import PythonScriptStep\n",
    "\n",
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "from azureml.core.runconfig import RunConfiguration\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "\n",
    "from azureml.core.model import Model\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print('SDK Version:', azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e920807",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Workspace name: hw2_pipeline\n",
      "Azure region: centralus\n",
      "Subscription id: 441cf665-f2bb-42da-9467-9dc54a3ae431\n",
      "Resource group: pipeline\n"
     ]
    }
   ],
   "source": [
    "# load workspace\n",
    "ws = Workspace.from_config()\n",
    "print('Workspace name: ' + ws.name, \n",
    "      'Azure region: ' + ws.location, \n",
    "      'Subscription id: ' + ws.subscription_id, \n",
    "      'Resource group: ' + ws.resource_group, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1afbbbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "azureml_globaldatasets - Default = False\n",
      "workspaceblobstore - Default = True\n",
      "workspacefilestore - Default = False\n",
      "workspaceworkingdirectory - Default = False\n",
      "workspaceartifactstore - Default = False\n"
     ]
    }
   ],
   "source": [
    "# Get the default datastore\n",
    "default_ds = ws.get_default_datastore()\n",
    "\n",
    "# Enumerate all datastores, indicating which is the default\n",
    "for ds_name in ws.datastores:\n",
    "    print(ds_name, \"- Default =\", ds_name == default_ds.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "481c5be2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already registered.\n"
     ]
    }
   ],
   "source": [
    "if 'churn dataset' not in ws.datasets:\n",
    "    default_ds.upload_files(files=['/Users/lingyizhao/Desktop/new_data.csv'], # Upload the diabetes csv files in /data\n",
    "                        target_path='churn-data/', # Put it in a folder path in the datastore\n",
    "                        overwrite=True, # Replace existing files of the same name\n",
    "                        show_progress=True)\n",
    "\n",
    "    #Create a tabular dataset from the path on the datastore (this may take a short while)\n",
    "    tab_data_set = Dataset.Tabular.from_delimited_files(path=(default_ds, 'churn-data/new_data.csv'))\n",
    "\n",
    "    # Register the tabular dataset\n",
    "    try:\n",
    "        tab_data_set = tab_data_set.register(workspace=ws, \n",
    "                                name='churn dataset',\n",
    "                                description='churn data',\n",
    "                                tags = {'format':'CSV'},\n",
    "                                create_new_version=True)\n",
    "        print('Dataset registered.')\n",
    "    except Exception as ex:\n",
    "        print(ex)\n",
    "else:\n",
    "    print('Dataset already registered.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fac7ce16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "churn_pipeline\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# Create a folder for the pipeline step files\n",
    "experiment_folder = 'churn_pipeline'\n",
    "os.makedirs(experiment_folder, exist_ok=True)\n",
    "\n",
    "print(experiment_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69706320",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing churn_pipeline/prep_churn.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $experiment_folder/prep_churn.py\n",
    "# Import libraries\n",
    "import os\n",
    "import argparse\n",
    "import pandas as pd\n",
    "from azureml.core import Run\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Get parameters\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--input-data\", type=str, dest='raw_dataset_id', help='raw dataset')\n",
    "parser.add_argument('--prepped-data', type=str, dest='prepped_data', default='prepped_data', help='Folder for results')\n",
    "args = parser.parse_args()\n",
    "save_folder = args.prepped_data\n",
    "\n",
    "# Get the experiment run context\n",
    "run = Run.get_context()\n",
    "\n",
    "# load the data (passed as an input dataset)\n",
    "print(\"Loading Data...\")\n",
    "churn = run.input_datasets['raw_data'].to_pandas_dataframe()\n",
    "\n",
    "# Log raw row count\n",
    "row_count = (len(churn))\n",
    "run.log('raw_rows', row_count)\n",
    "\n",
    "# data cleaning\n",
    "churn = churn.dropna()\n",
    "\n",
    "# Log processed rows\n",
    "row_count = (len(churn))\n",
    "run.log('processed_rows', row_count)\n",
    "\n",
    "# Save the prepped data\n",
    "print(\"Saving Data...\")\n",
    "os.makedirs(save_folder, exist_ok=True)\n",
    "save_path = os.path.join(save_folder,'data.csv')\n",
    "churn.to_csv(save_path, index=False, header=True)\n",
    "\n",
    "# End the run\n",
    "run.complete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "caadd252",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing churn_pipeline/train_churn.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $experiment_folder/train_churn.py\n",
    "# Import libraries\n",
    "from azureml.core import Run, Model\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get parameters\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--training-folder\", type=str, dest='training_folder', help='training data folder')\n",
    "args = parser.parse_args()\n",
    "training_folder = args.training_folder\n",
    "\n",
    "# Get the experiment run context\n",
    "run = Run.get_context()\n",
    "\n",
    "# load the prepared data file in the training folder\n",
    "print(\"Loading Data...\")\n",
    "file_path = os.path.join(training_folder,'data.csv')\n",
    "churn = pd.read_csv(file_path)\n",
    "\n",
    "X = churn[churn.columns.difference(['Attrition_Yes'])].values\n",
    "y = churn['Attrition_Yes'].values\n",
    "\n",
    "# Split data into training set and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=0)\n",
    "\n",
    "# Scale the data, using standardization\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "# Train the random forest model\n",
    "print('Training a random forest model...')\n",
    "model = RandomForestClassifier().fit(X_train, y_train)\n",
    "\n",
    "# calculate accuracy\n",
    "y_hat = model.predict(X_test)\n",
    "acc = np.average(y_hat == y_test)\n",
    "print('Accuracy:', acc)\n",
    "run.log('Accuracy', np.float(acc))\n",
    "\n",
    "# calculate AUC\n",
    "y_scores = model.predict_proba(X_test)\n",
    "auc = roc_auc_score(y_test,y_scores[:,1])\n",
    "print('AUC: ' + str(auc))\n",
    "run.log('AUC', np.float(auc))\n",
    "\n",
    "# plot ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_scores[:,1])\n",
    "fig = plt.figure(figsize=(6, 4))\n",
    "# Plot the diagonal 50% line\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "# Plot the FPR and TPR achieved by our model\n",
    "plt.plot(fpr, tpr)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "run.log_image(name = \"ROC\", plot = fig)\n",
    "plt.show()\n",
    "\n",
    "# Save the trained model in the outputs folder\n",
    "print(\"Saving model...\")\n",
    "os.makedirs('outputs', exist_ok=True)\n",
    "model_file = os.path.join('outputs', 'randomforest_model.pkl')\n",
    "joblib.dump(value=model, filename=model_file)\n",
    "\n",
    "# Register the model\n",
    "print('Registering model...')\n",
    "Model.register(workspace=run.experiment.workspace,\n",
    "               model_path = model_file,\n",
    "               model_name = 'randomforest_model',\n",
    "               tags={'Training context':'Pipeline'},\n",
    "               properties={'AUC': np.float(auc), 'Accuracy': np.float(acc)})\n",
    "\n",
    "\n",
    "run.complete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "caa8c349",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing cluster, use it.\n"
     ]
    }
   ],
   "source": [
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "cluster_name = \"mlop-test\"\n",
    "\n",
    "try:\n",
    "    # Check for existing compute target\n",
    "    pipeline_cluster = ComputeTarget(workspace=ws, name=cluster_name)\n",
    "    print('Found existing cluster, use it.')\n",
    "except ComputeTargetException:\n",
    "    # If it doesn't already exist, create it\n",
    "    try:\n",
    "        compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_DS11_V2', max_nodes=2)\n",
    "        pipeline_cluster = ComputeTarget.create(ws, cluster_name, compute_config)\n",
    "        pipeline_cluster.wait_for_completion(show_output=True)\n",
    "    except Exception as ex:\n",
    "        print(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e75b8212",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run configuration created.\n"
     ]
    }
   ],
   "source": [
    "from azureml.core import Environment\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "from azureml.core.runconfig import RunConfiguration\n",
    "\n",
    "# Create a Python environment for the experiment\n",
    "churn_env = Environment(\"churn-pipeline-env\")\n",
    "\n",
    "# Create a set of package dependencies\n",
    "churn_packages = CondaDependencies.create(conda_packages=['scikit-learn','ipykernel','matplotlib','pandas','pip'],\n",
    "                                             pip_packages=['azureml-defaults','azureml-dataprep[pandas]','pyarrow'])\n",
    "\n",
    "# Add the dependencies to the environment\n",
    "churn_env.python.conda_dependencies = churn_packages\n",
    "\n",
    "# Register the environment \n",
    "churn_env.register(workspace=ws)\n",
    "registered_env = Environment.get(ws, 'churn-pipeline-env')\n",
    "\n",
    "# Create a new runconfig object for the pipeline\n",
    "pipeline_run_config = RunConfiguration()\n",
    "\n",
    "# Use the compute you created above. \n",
    "pipeline_run_config.target = pipeline_cluster\n",
    "\n",
    "# Assign the environment to the run configuration\n",
    "pipeline_run_config.environment = registered_env\n",
    "\n",
    "print (\"Run configuration created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7ce7cc1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline steps defined\n"
     ]
    }
   ],
   "source": [
    "from azureml.pipeline.core import PipelineData\n",
    "from azureml.pipeline.steps import PythonScriptStep\n",
    "\n",
    "# Get the training dataset\n",
    "churn_ds = ws.datasets.get(\"churn dataset\")\n",
    "\n",
    "# Create a PipelineData (temporary Data Reference) for the model folder\n",
    "prepped_data_folder = PipelineData(\"prepped_data_folder\", datastore=ws.get_default_datastore())\n",
    "\n",
    "# Step 1, Run the data prep script\n",
    "train_step = PythonScriptStep(name = \"Prepare Data\",\n",
    "                                source_directory = experiment_folder,\n",
    "                                script_name = \"prep_churn.py\",\n",
    "                                arguments = ['--input-data', churn_ds.as_named_input('raw_data'),\n",
    "                                             '--prepped-data', prepped_data_folder],\n",
    "                                outputs=[prepped_data_folder],\n",
    "                                compute_target = pipeline_cluster,\n",
    "                                runconfig = pipeline_run_config,\n",
    "                                allow_reuse = True)\n",
    "\n",
    "# Step 2, run the training script\n",
    "register_step = PythonScriptStep(name = \"Train and Register Model\",\n",
    "                                source_directory = experiment_folder,\n",
    "                                script_name = \"train_churn.py\",\n",
    "                                arguments = ['--training-folder', prepped_data_folder],\n",
    "                                inputs=[prepped_data_folder],\n",
    "                                compute_target = pipeline_cluster,\n",
    "                                runconfig = pipeline_run_config,\n",
    "                                allow_reuse = True)\n",
    "\n",
    "print(\"Pipeline steps defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d716d22a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline is built.\n",
      "Created step Prepare Data [8702eff8][04cfee9f-3773-49e2-9e65-179237aa0ca5], (This step will run and generate new outputs)Created step Train and Register Model [5b262813][a9bc6a29-edfc-41b6-9b6d-20d59340668c], (This step will run and generate new outputs)\n",
      "\n",
      "Submitted PipelineRun 9bc15bf9-734b-4c2f-9049-de538d8d444f\n",
      "Link to Azure Machine Learning Portal: https://ml.azure.com/runs/9bc15bf9-734b-4c2f-9049-de538d8d444f?wsid=/subscriptions/441cf665-f2bb-42da-9467-9dc54a3ae431/resourcegroups/pipeline/workspaces/hw2_pipeline&tid=6d7adc86-8446-4cb9-9972-1477c831ff28\n",
      "Pipeline submitted for execution.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cda9dab86cb742d0a73923e94b963232",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "_PipelineWidget(widget_settings={'childWidgetDisplay': 'popup', 'send_telemetry': False, 'log_level': 'INFO', â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/aml.mini.widget.v1": "{\"status\": \"Running\", \"workbench_run_details_uri\": \"https://ml.azure.com/runs/9bc15bf9-734b-4c2f-9049-de538d8d444f?wsid=/subscriptions/441cf665-f2bb-42da-9467-9dc54a3ae431/resourcegroups/pipeline/workspaces/hw2_pipeline&tid=6d7adc86-8446-4cb9-9972-1477c831ff28\", \"run_id\": \"9bc15bf9-734b-4c2f-9049-de538d8d444f\", \"run_properties\": {\"run_id\": \"9bc15bf9-734b-4c2f-9049-de538d8d444f\", \"created_utc\": \"2021-11-04T20:02:59.034162Z\", \"properties\": {\"azureml.runsource\": \"azureml.PipelineRun\", \"runSource\": \"SDK\", \"runType\": \"SDK\", \"azureml.parameters\": \"{}\", \"azureml.pipelineComponent\": \"pipelinerun\"}, \"tags\": {}, \"end_time_utc\": null, \"status\": \"Running\", \"log_files\": {\"logs/azureml/executionlogs.txt\": \"https://hw2pipeline7446083280.blob.core.windows.net/azureml/ExperimentRun/dcid.9bc15bf9-734b-4c2f-9049-de538d8d444f/logs/azureml/executionlogs.txt?sv=2019-07-07&sr=b&sig=NueGYTUuASdV7FoCiwrdCAGUz7NtKiBDDBgfhbtVQzY%3D&skoid=f9378829-dfb7-42de-8f6c-6d4577bab050&sktid=6d7adc86-8446-4cb9-9972-1477c831ff28&skt=2021-11-04T14%3A14%3A36Z&ske=2021-11-05T22%3A24%3A36Z&sks=b&skv=2019-07-07&st=2021-11-04T19%3A53%3A26Z&se=2021-11-05T04%3A03%3A26Z&sp=r\", \"logs/azureml/stderrlogs.txt\": \"https://hw2pipeline7446083280.blob.core.windows.net/azureml/ExperimentRun/dcid.9bc15bf9-734b-4c2f-9049-de538d8d444f/logs/azureml/stderrlogs.txt?sv=2019-07-07&sr=b&sig=xbrBNxU6HgdcQmzJ0%2BlnR5VPxYn3Jw4FczRUOwPqyQ0%3D&skoid=f9378829-dfb7-42de-8f6c-6d4577bab050&sktid=6d7adc86-8446-4cb9-9972-1477c831ff28&skt=2021-11-04T14%3A14%3A36Z&ske=2021-11-05T22%3A24%3A36Z&sks=b&skv=2019-07-07&st=2021-11-04T19%3A53%3A26Z&se=2021-11-05T04%3A03%3A26Z&sp=r\", \"logs/azureml/stdoutlogs.txt\": \"https://hw2pipeline7446083280.blob.core.windows.net/azureml/ExperimentRun/dcid.9bc15bf9-734b-4c2f-9049-de538d8d444f/logs/azureml/stdoutlogs.txt?sv=2019-07-07&sr=b&sig=KM4Euhg0w4L4Yj9rXbywtPbJb7Z5kbZaLNoWbkXbtbk%3D&skoid=f9378829-dfb7-42de-8f6c-6d4577bab050&sktid=6d7adc86-8446-4cb9-9972-1477c831ff28&skt=2021-11-04T14%3A14%3A36Z&ske=2021-11-05T22%3A24%3A36Z&sks=b&skv=2019-07-07&st=2021-11-04T19%3A53%3A26Z&se=2021-11-05T04%3A03%3A26Z&sp=r\"}, \"log_groups\": [[\"logs/azureml/executionlogs.txt\", \"logs/azureml/stderrlogs.txt\", \"logs/azureml/stdoutlogs.txt\"]], \"run_duration\": \"0:01:34\", \"run_number\": \"28\", \"run_queued_details\": {\"status\": \"Running\", \"details\": null}}, \"child_runs\": [{\"run_id\": \"af214cf5-8bda-4418-8cc8-d1dcf4813860\", \"name\": \"Prepare Data\", \"status\": \"Running\", \"start_time\": \"2021-11-04T20:03:31.260137Z\", \"created_time\": \"2021-11-04T20:03:01.607255Z\", \"end_time\": \"2021-11-04T20:04:02.253988Z\", \"duration\": \"0:01:00\", \"run_number\": 29, \"metric\": null, \"run_type\": \"azureml.StepRun\", \"training_percent\": null, \"created_time_dt\": \"2021-11-04T20:03:01.607255Z\", \"is_reused\": \"\"}, {\"run_id\": \"\", \"name\": \"Train and Register Model\", \"status\": \"NotStarted\", \"start_time\": \"\", \"created_time\": \"\", \"end_time\": \"\", \"duration\": \"\"}], \"children_metrics\": {\"categories\": null, \"series\": null, \"metricName\": null}, \"run_metrics\": [], \"run_logs\": \"[2021-11-04 20:03:01Z] Submitting 1 runs, first five are: 8702eff8:af214cf5-8bda-4418-8cc8-d1dcf4813860\\n\", \"graph\": {\"datasource_nodes\": {\"f7f20ba6\": {\"node_id\": \"f7f20ba6\", \"name\": \"churn dataset\"}}, \"module_nodes\": {\"8702eff8\": {\"node_id\": \"8702eff8\", \"name\": \"Prepare Data\", \"status\": \"Running\", \"_is_reused\": false, \"run_id\": \"af214cf5-8bda-4418-8cc8-d1dcf4813860\"}, \"5b262813\": {\"node_id\": \"5b262813\", \"name\": \"Train and Register Model\", \"status\": \"NotStarted\"}}, \"edges\": [{\"source_node_id\": \"f7f20ba6\", \"source_node_name\": \"churn dataset\", \"source_name\": \"data\", \"target_name\": \"raw_data\", \"dst_node_id\": \"8702eff8\", \"dst_node_name\": \"Prepare Data\"}, {\"source_node_id\": \"8702eff8\", \"source_node_name\": \"Prepare Data\", \"source_name\": \"prepped_data_folder\", \"target_name\": \"prepped_data_folder\", \"dst_node_id\": \"5b262813\", \"dst_node_name\": \"Train and Register Model\"}], \"child_runs\": [{\"run_id\": \"af214cf5-8bda-4418-8cc8-d1dcf4813860\", \"name\": \"Prepare Data\", \"status\": \"Running\", \"start_time\": \"2021-11-04T20:03:31.260137Z\", \"created_time\": \"2021-11-04T20:03:01.607255Z\", \"end_time\": \"2021-11-04T20:04:02.253988Z\", \"duration\": \"0:01:00\", \"run_number\": 29, \"metric\": null, \"run_type\": \"azureml.StepRun\", \"training_percent\": null, \"created_time_dt\": \"2021-11-04T20:03:01.607255Z\", \"is_reused\": \"\"}, {\"run_id\": \"\", \"name\": \"Train and Register Model\", \"status\": \"NotStarted\", \"start_time\": \"\", \"created_time\": \"\", \"end_time\": \"\", \"duration\": \"\"}]}, \"widget_settings\": {\"childWidgetDisplay\": \"popup\", \"send_telemetry\": false, \"log_level\": \"INFO\", \"sdk_version\": \"1.35.0\"}, \"loading\": false}"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PipelineRunId: 9bc15bf9-734b-4c2f-9049-de538d8d444f\n",
      "Link to Azure Machine Learning Portal: https://ml.azure.com/runs/9bc15bf9-734b-4c2f-9049-de538d8d444f?wsid=/subscriptions/441cf665-f2bb-42da-9467-9dc54a3ae431/resourcegroups/pipeline/workspaces/hw2_pipeline&tid=6d7adc86-8446-4cb9-9972-1477c831ff28\n",
      "PipelineRun Status: NotStarted\n",
      "PipelineRun Status: Running\n",
      "\n",
      "\n",
      "StepRunId: af214cf5-8bda-4418-8cc8-d1dcf4813860\n",
      "Link to Azure Machine Learning Portal: https://ml.azure.com/runs/af214cf5-8bda-4418-8cc8-d1dcf4813860?wsid=/subscriptions/441cf665-f2bb-42da-9467-9dc54a3ae431/resourcegroups/pipeline/workspaces/hw2_pipeline&tid=6d7adc86-8446-4cb9-9972-1477c831ff28\n",
      "StepRun( Prepare Data ) Status: Running\n",
      "\n",
      "Streaming azureml-logs/55_azureml-execution-tvmps_53bb39c91e5339d39dd46dc46ed72174a017c78327fbfa2f7f38d3d35f9977ea_d.txt\n",
      "========================================================================================================================\n",
      "2021-11-04T20:03:32Z Running following command: /bin/bash -c sudo blobfuse /mnt/batch/tasks/shared/LS_root/jobs/hw2_pipeline/azureml/af214cf5-8bda-4418-8cc8-d1dcf4813860/mounts/workspaceblobstore --tmp-path=/mnt/batch/tasks/shared/LS_root/jobs/hw2_pipeline/azureml/af214cf5-8bda-4418-8cc8-d1dcf4813860/caches/workspaceblobstore --file-cache-timeout-in-seconds=1000000 --cache-size-mb=21863 -o nonempty -o allow_other --config-file=/mnt/batch/tasks/shared/LS_root/jobs/hw2_pipeline/azureml/af214cf5-8bda-4418-8cc8-d1dcf4813860/configs/workspaceblobstore.cfg --log-level=LOG_WARNING\n",
      "2021-11-04T20:03:32Z Successfully mounted a/an Blobfuse File System at /mnt/batch/tasks/shared/LS_root/jobs/hw2_pipeline/azureml/af214cf5-8bda-4418-8cc8-d1dcf4813860/mounts/workspaceblobstore\n",
      "2021-11-04T20:03:32Z The vmsize standard_ds11_v2 is not a GPU VM, skipping get GPU count by running nvidia-smi command.\n",
      "2021-11-04T20:03:32Z Starting output-watcher...\n",
      "2021-11-04T20:03:32Z IsDedicatedCompute == True, won't poll for Low Pri Preemption\n",
      "2021-11-04T20:03:33Z Executing 'Copy ACR Details file' on 10.0.0.4\n",
      "2021-11-04T20:03:33Z Copy ACR Details file succeeded on 10.0.0.4. Output: \n",
      ">>>   \n",
      ">>>   \n",
      "Login Succeeded\n",
      "Using default tag: latest\n",
      "latest: Pulling from azureml/azureml_74b5d7b1bfec63ece12721fc9d263e9a\n",
      "Digest: sha256:b38056d72f7574d4ce9412e84dedd30e2f5025b046ecc28854f5dafcef12c78b\n",
      "Status: Image is up to date for 96758eaafa3647d4a83e17ccee49d173.azurecr.io/azureml/azureml_74b5d7b1bfec63ece12721fc9d263e9a:latest\n",
      "96758eaafa3647d4a83e17ccee49d173.azurecr.io/azureml/azureml_74b5d7b1bfec63ece12721fc9d263e9a:latest\n",
      "2021-11-04T20:03:33Z The vmsize standard_ds11_v2 is not a GPU VM, skipping get GPU count by running nvidia-smi command.\n",
      "2021-11-04T20:03:33Z Check if container af214cf5-8bda-4418-8cc8-d1dcf4813860 already exist exited with 0, \n",
      "\n",
      "7a9fb636caf9d2f5f51b4b36e275bd3e587535afbd3c378aa100a44a13fb43b6\n",
      "2021-11-04T20:03:33Z Parameters for containerSetup task: useDetonationChamer set to false and sshRequired set to false \n",
      "2021-11-04T20:03:33Z containerSetup task cmd: [/mnt/batch/tasks/startup/wd/hosttools -task=containerSetup -traceContext=00-0ae1445cbfb117efbbfd80bb82b419d2-aa8db9e3f358b901-01 -sshRequired=false] \n",
      "2021/11/04 20:03:33 Got JobInfoJson from env\n",
      "2021/11/04 20:03:33 Starting App Insight Logger for task:  containerSetup\n",
      "2021/11/04 20:03:33 Version: 3.0.01755.0003 Branch: .SourceBranch Commit: 66828d8\n",
      "2021/11/04 20:03:33 Entered ContainerSetupTask - Preparing infiniband\n",
      "2021/11/04 20:03:33 Starting infiniband setup\n",
      "2021/11/04 20:03:33 Python Version found is Python 3.6.2 :: Anaconda, Inc.\n",
      "\n",
      "2021/11/04 20:03:33 Returning Python Version as 3.6\n",
      "2021/11/04 20:03:33 VMSize: standard_ds11_v2, Host: runtime-gen1-ubuntu18, Container: ubuntu-18.04\n",
      "2021/11/04 20:03:33 VMSize: standard_ds11_v2, Host: runtime-gen1-ubuntu18, Container: ubuntu-18.04\n",
      "2021-11-04T20:03:33Z VMSize: standard_ds11_v2, Host: runtime-gen1-ubuntu18, Container: ubuntu-18.04\n",
      "2021/11/04 20:03:33 /dev/infiniband/uverbs0 found (implying presence of InfiniBand)?: false\n",
      "2021/11/04 20:03:33 Not setting up Infiniband in Container\n",
      "2021/11/04 20:03:33 Not setting up Infiniband in Container\n",
      "2021-11-04T20:03:33Z Not setting up Infiniband in Container\n",
      "2021/11/04 20:03:33 Python Version found is Python 3.6.2 :: Anaconda, Inc.\n",
      "\n",
      "2021/11/04 20:03:33 Returning Python Version as 3.6\n",
      "2021/11/04 20:03:33 sshd inside container not required for job, skipping setup.\n",
      "2021/11/04 20:03:34 All App Insights Logs was sent successfully or the close timeout of 10 was reached\n",
      "2021/11/04 20:03:34 App Insight Client has already been closed\n",
      "2021/11/04 20:03:34 Not exporting to RunHistory as the exporter is either stopped or there is no data.\n",
      "Stopped: false\n",
      "OriginalData: 1\n",
      "FilteredData: 0.\n",
      "2021-11-04T20:03:34Z Starting docker container succeeded.\n",
      "2021-11-04T20:03:37Z The vmsize standard_ds11_v2 is not a GPU VM, skipping get GPU count by running nvidia-smi command.\n",
      "2021-11-04T20:03:37Z The vmsize standard_ds11_v2 is not a GPU VM, skipping get GPU count by running nvidia-smi command.\n",
      "2021-11-04T20:03:38Z Job environment preparation succeeded on 10.0.0.4. Output: \n",
      ">>>   2021/11/04 20:03:31 Got JobInfoJson from env\n",
      ">>>   2021/11/04 20:03:31 Starting App Insight Logger for task:  prepareJobEnvironment\n",
      ">>>   2021/11/04 20:03:31 Version: 3.0.01755.0003 Branch: .SourceBranch Commit: 66828d8\n",
      ">>>   2021/11/04 20:03:31 Got JobInfoJson from env\n",
      ">>>   2021/11/04 20:03:31 runtime.GOOS linux\n",
      ">>>   2021/11/04 20:03:31 Checking if '/tmp' exists\n",
      ">>>   2021/11/04 20:03:31 Reading dyanamic configs\n",
      ">>>   2021/11/04 20:03:31 Container sas url: https://baiscriptsdm1prod.blob.core.windows.net/aihosttools?sv=2018-03-28&sr=c&si=aihosttoolspolicy&sig=0dYPjOcglC72KKRE2ILzFCKnPug7ECc2SIRWA3udLW0%3D\n",
      ">>>   2021/11/04 20:03:32 Starting Azsecpack installation on machine: e452e77d3e2f44698f1de6e630291a79000000#6d7adc86-8446-4cb9-9972-1477c831ff28#441cf665-f2bb-42da-9467-9dc54a3ae431#pipeline#hw2_pipeline#mlop-test#tvmps_53bb39c91e5339d39dd46dc46ed72174a017c78327fbfa2f7f38d3d35f9977ea_d\n",
      ">>>   2021/11/04 20:03:32 Failed to read from file /mnt/batch/tasks/startup/wd/az_resource/azsecpack.variables, open /mnt/batch/tasks/startup/wd/az_resource/azsecpack.variables: no such file or directory\n",
      ">>>   2021/11/04 20:03:32 Azsecpack installation directory: /mnt/batch/tasks/startup/wd/az_resource, Is Azsecpack installer on host: true. Is Azsecpack installation enabled: false,\n",
      ">>>   2021/11/04 20:03:32 Is Azsecpack enabled: false, GetDisableVsatlsscan: true\n",
      ">>>   2021/11/04 20:03:32 Turning off azsecpack, if it is already running\n",
      ">>>   2021/11/04 20:03:32 Start deleting Azsecpack installation cronjob...\n",
      ">>>   2021/11/04 20:03:32 Start checking if Azsecpack is running...\n",
      ">>>   2021/11/04 20:03:32 Azsecpack is not running. No need to stop Azsecpack processes.\n",
      ">>>   2021/11/04 20:03:32 bypass systemd resolved\n",
      ">>>   2021/11/04 20:03:32 Cluster Subscription Id: 441cf665-f2bb-42da-9467-9dc54a3ae431\n",
      ">>>   2021/11/04 20:03:32 Cluster Workspace Name: hw2_pipeline\n",
      ">>>   2021/11/04 20:03:32 Cluster Name: mlop-test\n",
      ">>>   2021/11/04 20:03:32 VMsize: standard_ds11_v2\n",
      ">>>   2021/11/04 20:03:32 GPU Count: 0\n",
      ">>>   2021/11/04 20:03:32 Job: AZ_BATCHAI_JOB_NAME does not turn on the DetonationChamber\n",
      ">>>   2021/11/04 20:03:32 The vmsize standard_ds11_v2 is not a GPU VM, skipping get GPU count by running nvidia-smi command.\n",
      ">>>   2021/11/04 20:03:32 The vmsize standard_ds11_v2 is not a GPU VM, skipping get GPU count by running nvidia-smi command.\n",
      ">>>   2021/11/04 20:03:32 Get GPU count failed with err: The vmsize standard_ds11_v2 is not a GPU VM, skipping get GPU count by running nvidia-smi command., \n",
      ">>>   2021/11/04 20:03:32 AMLComputeXDSEndpoint:  https://centralus.cert.api.azureml.ms/xdsbatchai\n",
      ">>>   2021/11/04 20:03:32 AMLComputeXDSApiVersion:  2018-02-01\n",
      ">>>   2021/11/04 20:03:32 Creating directory /mnt/batch/tasks/shared/LS_root/jobs/hw2_pipeline/azureml/af214cf5-8bda-4418-8cc8-d1dcf4813860/config\n",
      ">>>   2021/11/04 20:03:32 This is not a aml-workstation (compute instance), current offer type: amlcompute. Starting identity responder as part of prepareJobEnvironment.\n",
      ">>>   2021/11/04 20:03:32 Starting identity responder.\n",
      ">>>   2021/11/04 20:03:32 Starting identity responder.\n",
      ">>>   2021/11/04 20:03:32 Logfile used for identity responder: /mnt/batch/tasks/workitems/67279c7f-ca32-4555-9f75-59da8a4c4229/job-1/af214cf5-8bda-4418-8_9add3bc3-d046-4e33-8436-d92e35c12e85/IdentityResponderLog-tvmps_53bb39c91e5339d39dd46dc46ed72174a017c78327fbfa2f7f38d3d35f9977ea_d.txt\n",
      ">>>   2021/11/04 20:03:32 Logfile used for identity responder: /mnt/batch/tasks/workitems/67279c7f-ca32-4555-9f75-59da8a4c4229/job-1/af214cf5-8bda-4418-8_9add3bc3-d046-4e33-8436-d92e35c12e85/IdentityResponderLog-tvmps_53bb39c91e5339d39dd46dc46ed72174a017c78327fbfa2f7f38d3d35f9977ea_d.txt\n",
      ">>>   2021/11/04 20:03:32 Started Identity Responder for job.\n",
      ">>>   2021/11/04 20:03:32 Started Identity Responder for job.\n",
      ">>>   2021/11/04 20:03:32 Creating directory /mnt/batch/tasks/shared/LS_root/jobs/hw2_pipeline/azureml/af214cf5-8bda-4418-8cc8-d1dcf4813860/wd\n",
      ">>>   2021/11/04 20:03:32 Creating directory /mnt/batch/tasks/shared/LS_root/jobs/hw2_pipeline/azureml/af214cf5-8bda-4418-8cc8-d1dcf4813860/shared\n",
      ">>>   2021/11/04 20:03:32 WorkingDirPath is specified. Setting env AZ_BATCHAI_JOB_WORK_DIR=$AZ_BATCHAI_JOB_TEMP/azureml/af214cf5-8bda-4418-8cc8-d1dcf4813860\n",
      ">>>   2021/11/04 20:03:32 From the policy service, the filtering patterns is: , data store is \n",
      ">>>   2021/11/04 20:03:32 Mounting job level file systems\n",
      ">>>   2021/11/04 20:03:32 Creating directory /mnt/batch/tasks/shared/LS_root/jobs/hw2_pipeline/azureml/af214cf5-8bda-4418-8cc8-d1dcf4813860/mounts\n",
      ">>>   2021/11/04 20:03:32 Attempting to read datastore credentials file: /mnt/batch/tasks/shared/LS_root/jobs/hw2_pipeline/azureml/af214cf5-8bda-4418-8cc8-d1dcf4813860/config/.amlcompute.datastorecredentials\n",
      ">>>   2021/11/04 20:03:32 Datastore credentials file not found, skipping.\n",
      ">>>   2021/11/04 20:03:32 Attempting to read runtime sas tokens file: /mnt/batch/tasks/shared/LS_root/jobs/hw2_pipeline/azureml/af214cf5-8bda-4418-8cc8-d1dcf4813860/config/.master.runtimesastokens\n",
      ">>>   2021/11/04 20:03:32 Runtime sas tokens file not found, skipping.\n",
      ">>>   2021/11/04 20:03:32 NFS mount is not enabled\n",
      ">>>   2021/11/04 20:03:32 No Azure File Shares configured\n",
      ">>>   2021/11/04 20:03:32 Mounting blob file systems\n",
      ">>>   2021/11/04 20:03:32 Blobfuse runtime version 1.3.6\n",
      ">>>   2021/11/04 20:03:32 Mounting azureml-blobstore-96758eaa-fa36-47d4-a83e-17ccee49d173 container from hw2pipeline7446083280 account at /mnt/batch/tasks/shared/LS_root/jobs/hw2_pipeline/azureml/af214cf5-8bda-4418-8cc8-d1dcf4813860/mounts/workspaceblobstore\n",
      ">>>   2021/11/04 20:03:32 Using Compute Identity to authenticate Blobfuse: false.\n",
      ">>>   2021/11/04 20:03:32 Using Compute Identity to authenticate Blobfuse: false.\n",
      ">>>   2021/11/04 20:03:32 Blobfuse cache size set to 21863 MB.\n",
      ">>>   2021/11/04 20:03:32 Running following command: /bin/bash -c sudo blobfuse /mnt/batch/tasks/shared/LS_root/jobs/hw2_pipeline/azureml/af214cf5-8bda-4418-8cc8-d1dcf4813860/mounts/workspaceblobstore --tmp-path=/mnt/batch/tasks/shared/LS_root/jobs/hw2_pipeline/azureml/af214cf5-8bda-4418-8cc8-d1dcf4813860/caches/workspaceblobstore --file-cache-timeout-in-seconds=1000000 --cache-size-mb=21863 -o nonempty -o allow_other --config-file=/mnt/batch/tasks/shared/LS_root/jobs/hw2_pipeline/azureml/af214cf5-8bda-4418-8cc8-d1dcf4813860/configs/workspaceblobstore.cfg --log-level=LOG_WARNING\n",
      ">>>   2021/11/04 20:03:32 Successfully mounted a/an Blobfuse File System at /mnt/batch/tasks/shared/LS_root/jobs/hw2_pipeline/azureml/af214cf5-8bda-4418-8cc8-d1dcf4813860/mounts/workspaceblobstore\n",
      ">>>   2021/11/04 20:03:32 Waiting for blobfs to be mounted at /mnt/batch/tasks/shared/LS_root/jobs/hw2_pipeline/azureml/af214cf5-8bda-4418-8cc8-d1dcf4813860/mounts/workspaceblobstore\n",
      ">>>   2021/11/04 20:03:32 Successfully mounted azureml-blobstore-96758eaa-fa36-47d4-a83e-17ccee49d173 container from hw2pipeline7446083280 account at /mnt/batch/tasks/shared/LS_root/jobs/hw2_pipeline/azureml/af214cf5-8bda-4418-8cc8-d1dcf4813860/mounts/workspaceblobstore\n",
      ">>>   2021/11/04 20:03:32 Created run_id directory: /mnt/batch/tasks/shared/LS_root/jobs/hw2_pipeline/azureml/af214cf5-8bda-4418-8cc8-d1dcf4813860/mounts/workspaceblobstore/azureml/af214cf5-8bda-4418-8cc8-d1dcf4813860\n",
      ">>>   2021/11/04 20:03:32 No unmanaged file systems configured\n",
      ">>>   2021/11/04 20:03:32 The vmsize standard_ds11_v2 is not a GPU VM, skipping get GPU count by running nvidia-smi command.\n",
      ">>>   2021/11/04 20:03:32 The vmsize standard_ds11_v2 is not a GPU VM, skipping get GPU count by running nvidia-smi command.\n",
      ">>>   2021/11/04 20:03:32 WorkingDirPath is specified. Setting env AZ_BATCHAI_JOB_WORK_DIR=$AZ_BATCHAI_JOB_TEMP/azureml/af214cf5-8bda-4418-8cc8-d1dcf4813860\n",
      ">>>   2021/11/04 20:03:32 From the policy service, the filtering patterns is: , data store is \n",
      ">>>   2021/11/04 20:03:32 Creating working directory: /mnt/batch/tasks/shared/LS_root/jobs/hw2_pipeline/azureml/af214cf5-8bda-4418-8cc8-d1dcf4813860/wd/azureml/af214cf5-8bda-4418-8cc8-d1dcf4813860\n",
      ">>>   2021/11/04 20:03:32 Creating directory /mnt/batch/tasks/shared/LS_root/jobs/hw2_pipeline/azureml/af214cf5-8bda-4418-8cc8-d1dcf4813860/wd/azureml/af214cf5-8bda-4418-8cc8-d1dcf4813860\n",
      ">>>   2021/11/04 20:03:32 Changing permissions for all existing files under directory: /mnt/batch/tasks/shared/LS_root/jobs/hw2_pipeline/azureml/af214cf5-8bda-4418-8cc8-d1dcf4813860/wd/azureml/af214cf5-8bda-4418-8cc8-d1dcf4813860\n",
      ">>>   2021/11/04 20:03:32 Change mode to 777 for dir /mnt/batch/tasks/shared/LS_root/jobs/hw2_pipeline/azureml/af214cf5-8bda-4418-8cc8-d1dcf4813860/wd/azureml/af214cf5-8bda-4418-8cc8-d1dcf4813860\n",
      ">>>   2021/11/04 20:03:32 Change mode to 777 for dir /mnt/batch/tasks/shared/LS_root/jobs/hw2_pipeline/azureml/af214cf5-8bda-4418-8cc8-d1dcf4813860/wd/azureml/af214cf5-8bda-4418-8cc8-d1dcf4813860/azureml_compute_logs\n",
      ">>>   2021/11/04 20:03:32 Change mode to 777 for dir /mnt/batch/tasks/shared/LS_root/jobs/hw2_pipeline/azureml/af214cf5-8bda-4418-8cc8-d1dcf4813860/wd/azureml/af214cf5-8bda-4418-8cc8-d1dcf4813860/azureml_compute_logs/tvmps_53bb39c91e5339d39dd46dc46ed72174a017c78327fbfa2f7f38d3d35f9977ea_d\n",
      ">>>   2021/11/04 20:03:32 Change mode to 666 for file /mnt/batch/tasks/shared/LS_root/jobs/hw2_pipeline/azureml/af214cf5-8bda-4418-8cc8-d1dcf4813860/wd/azureml/af214cf5-8bda-4418-8cc8-d1dcf4813860/azureml_compute_logs/tvmps_53bb39c91e5339d39dd46dc46ed72174a017c78327fbfa2f7f38d3d35f9977ea_d/55_azureml-execution-tvmps_53bb39c91e5339d39dd46dc46ed72174a017c78327fbfa2f7f38d3d35f9977ea_d.txt\n",
      ">>>   2021/11/04 20:03:32 Set default ACL for files under directory by running: /usr/bin/setfacl -m default:g::rwx -m default:o::rwx /mnt/batch/tasks/shared/LS_root/jobs/hw2_pipeline/azureml/af214cf5-8bda-4418-8cc8-d1dcf4813860/wd/azureml/af214cf5-8bda-4418-8cc8-d1dcf4813860\n",
      ">>>   2021/11/04 20:03:32 Creating directory /mnt/batch/tasks/shared/LS_root/jobs/hw2_pipeline/azureml/af214cf5-8bda-4418-8cc8-d1dcf4813860/wd/azureml/af214cf5-8bda-4418-8cc8-d1dcf4813860/azureml_compute_logs\n",
      ">>>   2021/11/04 20:03:32 Changing permissions for all existing files under directory: /mnt/batch/tasks/shared/LS_root/jobs/hw2_pipeline/azureml/af214cf5-8bda-4418-8cc8-d1dcf4813860/wd/azureml/af214cf5-8bda-4418-8cc8-d1dcf4813860/azureml_compute_logs\n",
      ">>>   2021/11/04 20:03:32 Change mode to 777 for dir /mnt/batch/tasks/shared/LS_root/jobs/hw2_pipeline/azureml/af214cf5-8bda-4418-8cc8-d1dcf4813860/wd/azureml/af214cf5-8bda-4418-8cc8-d1dcf4813860/azureml_compute_logs\n",
      ">>>   2021/11/04 20:03:32 Change mode to 777 for dir /mnt/batch/tasks/shared/LS_root/jobs/hw2_pipeline/azureml/af214cf5-8bda-4418-8cc8-d1dcf4813860/wd/azureml/af214cf5-8bda-4418-8cc8-d1dcf4813860/azureml_compute_logs/tvmps_53bb39c91e5339d39dd46dc46ed72174a017c78327fbfa2f7f38d3d35f9977ea_d\n",
      ">>>   2021/11/04 20:03:32 Change mode to 666 for file /mnt/batch/tasks/shared/LS_root/jobs/hw2_pipeline/azureml/af214cf5-8bda-4418-8cc8-d1dcf4813860/wd/azureml/af214cf5-8bda-4418-8cc8-d1dcf4813860/azureml_compute_logs/tvmps_53bb39c91e5339d39dd46dc46ed72174a017c78327fbfa2f7f38d3d35f9977ea_d/55_azureml-execution-tvmps_53bb39c91e5339d39dd46dc46ed72174a017c78327fbfa2f7f38d3d35f9977ea_d.txt\n",
      ">>>   2021/11/04 20:03:32 Set default ACL for files under directory by running: /usr/bin/setfacl -m default:g::rwx -m default:o::rwx /mnt/batch/tasks/shared/LS_root/jobs/hw2_pipeline/azureml/af214cf5-8bda-4418-8cc8-d1dcf4813860/wd/azureml/af214cf5-8bda-4418-8cc8-d1dcf4813860/azureml_compute_logs\n",
      ">>>   2021/11/04 20:03:32 Creating directory /mnt/batch/tasks/shared/LS_root/jobs/hw2_pipeline/azureml/af214cf5-8bda-4418-8cc8-d1dcf4813860/wd/azureml/af214cf5-8bda-4418-8cc8-d1dcf4813860/logs\n",
      ">>>   2021/11/04 20:03:32 Creating directory /mnt/batch/tasks/shared/LS_root/jobs/hw2_pipeline/azureml/af214cf5-8bda-4418-8cc8-d1dcf4813860/wd/azureml/af214cf5-8bda-4418-8cc8-d1dcf4813860/outputs\n",
      ">>>   2021/11/04 20:03:32 Starting output-watcher...\n",
      ">>>   2021/11/04 20:03:32 Single file input dataset is enabled.\n",
      ">>>   2021/11/04 20:03:32 SidecarEnabled:: isDetonationChamber: false, useDockerContainer: true\n",
      ">>>   2021/11/04 20:03:32 SidecarEnabled:: AmlDatasetContextManagerConfig exists: false\n",
      ">>>   2021/11/04 20:03:32 SidecarEnabled:: sidecar not enabled\n",
      ">>>   2021/11/04 20:03:32 Start to pulling docker image: 96758eaafa3647d4a83e17ccee49d173.azurecr.io/azureml/azureml_74b5d7b1bfec63ece12721fc9d263e9a\n",
      ">>>   2021/11/04 20:03:32 Start pull docker image: 96758eaafa3647d4a83e17ccee49d173.azurecr.io\n",
      ">>>   2021/11/04 20:03:32 Getting credentials for image 96758eaafa3647d4a83e17ccee49d173.azurecr.io/azureml/azureml_74b5d7b1bfec63ece12721fc9d263e9a with url 96758eaafa3647d4a83e17ccee49d173.azurecr.io\n",
      ">>>   2021/11/04 20:03:32 Container registry is ACR.\n",
      ">>>   2021/11/04 20:03:32 Skip getting ACR Credentials from Identity and will be getting it from EMS\n",
      ">>>   2021/11/04 20:03:32 Getting ACR Credentials from EMS for environment churn-pipeline-env:1\n",
      ">>>   2021/11/04 20:03:32 Requesting XDS for registry details.\n",
      ">>>   2021/11/04 20:03:32 Attempt 1 of http call to https://centralus.cert.api.azureml.ms/xdsbatchai/hosttoolapi/subscriptions/441cf665-f2bb-42da-9467-9dc54a3ae431/resourceGroups/pipeline/workspaces/hw2_pipeline/clusters/mlop-test/nodes/tvmps_53bb39c91e5339d39dd46dc46ed72174a017c78327fbfa2f7f38d3d35f9977ea_d?api-version=2018-02-01\n",
      ">>>   2021/11/04 20:03:33 Got container registry details from credentials service for registry address: 96758eaafa3647d4a83e17ccee49d173.azurecr.io.\n",
      ">>>   2021/11/04 20:03:33 Writing ACR Details to file...\n",
      ">>>   2021/11/04 20:03:33 Copying ACR Details file to worker nodes...\n",
      ">>>   2021/11/04 20:03:33 Executing 'Copy ACR Details file' on 10.0.0.4\n",
      ">>>   2021/11/04 20:03:33 Begin executing 'Copy ACR Details file' task on Node\n",
      ">>>   2021/11/04 20:03:33 'Copy ACR Details file' task Node result: succeeded\n",
      ">>>   2021/11/04 20:03:33 Copy ACR Details file succeeded on 10.0.0.4. Output: \n",
      ">>>   >>>   \n",
      ">>>   >>>   \n",
      ">>>   2021/11/04 20:03:33 Successfully retrieved ACR Credentials from EMS.\n",
      ">>>   2021/11/04 20:03:33 EMS returned 96758eaafa3647d4a83e17ccee49d173.azurecr.io for environment churn-pipeline-env\n",
      ">>>   2021/11/04 20:03:33 Save docker credentials for image 96758eaafa3647d4a83e17ccee49d173.azurecr.io/azureml/azureml_74b5d7b1bfec63ece12721fc9d263e9a in /mnt/batch/tasks/shared/LS_root/jobs/hw2_pipeline/azureml/af214cf5-8bda-4418-8cc8-d1dcf4813860/wd/docker_login_BE1F65095F156759\n",
      ">>>   2021/11/04 20:03:33 Start login to the docker registry\n",
      ">>>   2021/11/04 20:03:33 Successfully logged into the docker registry.\n",
      ">>>   2021/11/04 20:03:33 Start run pull docker image command\n",
      ">>>   2021/11/04 20:03:33 Pull docker image succeeded.\n",
      ">>>   2021/11/04 20:03:33 Removed docker config dir /mnt/batch/tasks/shared/LS_root/jobs/hw2_pipeline/azureml/af214cf5-8bda-4418-8cc8-d1dcf4813860/wd/docker_login_BE1F65095F156759\n",
      ">>>   2021/11/04 20:03:33 Pull docker image time: 603.499281ms\n",
      ">>>   \n",
      ">>>   2021/11/04 20:03:33 Docker Version that this nodes use are: 19.03.14+azure\n",
      ">>>   \n",
      ">>>   2021/11/04 20:03:33 The vmsize standard_ds11_v2 is not a GPU VM, skipping get GPU count by running nvidia-smi command.\n",
      ">>>   2021/11/04 20:03:33 The vmsize standard_ds11_v2 is not a GPU VM, skipping get GPU count by running nvidia-smi command.\n",
      ">>>   2021/11/04 20:03:33 Setting the memory limit for docker container to be 13674 MB\n",
      ">>>   2021/11/04 20:03:33 The env variable file size is 40943 bytes\n",
      ">>>   2021/11/04 20:03:33 Creating parent cgroup 'af214cf5-8bda-4418-8cc8-d1dcf4813860' for Containers used in Job\n",
      ">>>   2021/11/04 20:03:33 Add parent cgroup 'af214cf5-8bda-4418-8cc8-d1dcf4813860' to container 'af214cf5-8bda-4418-8cc8-d1dcf4813860'\n",
      ">>>   2021/11/04 20:03:33 /dev/infiniband/uverbs0 found (implying presence of InfiniBand)?: false\n",
      ">>>   2021/11/04 20:03:33 Original Arguments: run,--ulimit,memlock=9223372036854775807,--ulimit,nofile=262144:262144,--cap-add,sys_ptrace,--name,af214cf5-8bda-4418-8cc8-d1dcf4813860,-v,/mnt/batch/tasks/shared/LS_root/mounts:/mnt/batch/tasks/shared/LS_root/mounts:rshared,-v,/mnt/batch/tasks/shared/LS_root/configs:/mnt/batch/tasks/shared/LS_root/configs,-v,/mnt/batch/tasks/shared/LS_root/shared:/mnt/batch/tasks/shared/LS_root/shared,-v,/mnt/batch/tasks/workitems/67279c7f-ca32-4555-9f75-59da8a4c4229/job-1/af214cf5-8bda-4418-8_9add3bc3-d046-4e33-8436-d92e35c12e85/certs:/mnt/batch/tasks/workitems/67279c7f-ca32-4555-9f75-59da8a4c4229/job-1/af214cf5-8bda-4418-8_9add3bc3-d046-4e33-8436-d92e35c12e85/certs,-v,/mnt/batch/tasks/startup:/mnt/batch/tasks/startup,-m,13674m,-v,/mnt/batch/tasks/shared/LS_root/jobs/hw2_pipeline/azureml/af214cf5-8bda-4418-8cc8-d1dcf4813860/wd/azureml/af214cf5-8bda-4418-8cc8-d1dcf4813860/azureml_compute_logs:/mnt/batch/tasks/shared/LS_root/jobs/hw2_pipeline/azureml/af214cf5-8bda-4418-8cc8-d1dcf4813860/wd/azureml/af214cf5-8bda-4418-8cc8-d1dcf4813860/azureml_compute_logs,-v,/mnt/batch/tasks/workitems/67279c7f-ca32-4555-9f75-59da8a4c4229/job-1/af214cf5-8bda-4418-8_9add3bc3-d046-4e33-8436-d92e35c12e85/wd:/mnt/batch/tasks/workitems/67279c7f-ca32-4555-9f75-59da8a4c4229/job-1/af214cf5-8bda-4418-8_9add3bc3-d046-4e33-8436-d92e35c12e85/wd,-v,/mnt/batch/tasks/shared/LS_root/jobs/hw2_pipeline/azureml/af214cf5-8bda-4418-8cc8-d1dcf4813860:/mnt/batch/tasks/shared/LS_root/jobs/hw2_pipeline/azureml/af214cf5-8bda-4418-8cc8-d1dcf4813860,-v,/mnt/batch/tasks/shared/LS_root/shared/tracing/af214cf5-8bda-4418-8cc8-d1dcf4813860/logs/azureml/tracing:/mnt/batch/tasks/shared/LS_root/shared/tracing/af214cf5-8bda-4418-8cc8-d1dcf4813860/logs/azureml/tracing,-w,/mnt/batch/tasks/shared/LS_root/jobs/hw2_pipeline/azureml/af214cf5-8bda-4418-8cc8-d1dcf4813860/wd,--expose,23,--env-file,/mnt/batch/tasks/shared/LS_root/jobs/hw2_pipeline/azureml/af214cf5-8bda-4418-8cc8-d1dcf4813860/config/.batchai.envlist,--cgroup-parent=/af214cf5-8bda-4418-8cc8-d1dcf4813860/,--shm-size,2g\n",
      ">>>   2021/11/04 20:03:33 the binding /mnt/batch/tasks/shared/LS_root/shared/tracing/af214cf5-8bda-4418-8cc8-d1dcf4813860/logs/azureml/tracing:/mnt/batch/tasks/shared/LS_root/shared/tracing/af214cf5-8bda-4418-8cc8-d1dcf4813860/logs/azureml/tracing is discarded as we already have /mnt/batch/tasks/shared/LS_root/shared:/mnt/batch/tasks/shared/LS_root/shared \n",
      ">>>   2021/11/04 20:03:33 the binding /mnt/batch/tasks/shared/LS_root/jobs/hw2_pipeline/azureml/af214cf5-8bda-4418-8cc8-d1dcf4813860/wd/azureml/af214cf5-8bda-4418-8cc8-d1dcf4813860/azureml_compute_logs:/mnt/batch/tasks/shared/LS_root/jobs/hw2_pipeline/azureml/af214cf5-8bda-4418-8cc8-d1dcf4813860/wd/azureml/af214cf5-8bda-4418-8cc8-d1dcf4813860/azureml_compute_logs is discarded as we already have /mnt/batch/tasks/shared/LS_root/jobs/hw2_pipeline/azureml/af214cf5-8bda-4418-8cc8-d1dcf4813860:/mnt/batch/tasks/shared/LS_root/jobs/hw2_pipeline/azureml/af214cf5-8bda-4418-8cc8-d1dcf4813860 \n",
      ">>>   2021/11/04 20:03:33 Updated Arguments: run,--ulimit,memlock=9223372036854775807,--ulimit,nofile=262144:262144,--cap-add,sys_ptrace,--name,af214cf5-8bda-4418-8cc8-d1dcf4813860,-m,13674m,-w,/mnt/batch/tasks/shared/LS_root/jobs/hw2_pipeline/azureml/af214cf5-8bda-4418-8cc8-d1dcf4813860/wd,--expose,23,--env-file,/mnt/batch/tasks/shared/LS_root/jobs/hw2_pipeline/azureml/af214cf5-8bda-4418-8cc8-d1dcf4813860/config/.batchai.envlist,--cgroup-parent=/af214cf5-8bda-4418-8cc8-d1dcf4813860/,--shm-size,2g,-v,/mnt/batch/tasks/startup:/mnt/batch/tasks/startup,-v,/mnt/batch/tasks/shared/LS_root/mounts:/mnt/batch/tasks/shared/LS_root/mounts:rshared,-v,/mnt/batch/tasks/shared/LS_root/shared:/mnt/batch/tasks/shared/LS_root/shared,-v,/mnt/batch/tasks/shared/LS_root/configs:/mnt/batch/tasks/shared/LS_root/configs,-v,/mnt/batch/tasks/shared/LS_root/jobs/hw2_pipeline/azureml/af214cf5-8bda-4418-8cc8-d1dcf4813860:/mnt/batch/tasks/shared/LS_root/jobs/hw2_pipeline/azureml/af214cf5-8bda-4418-8cc8-d1dcf4813860,-v,/mnt/batch/tasks/workitems/67279c7f-ca32-4555-9f75-59da8a4c4229/job-1/af214cf5-8bda-4418-8_9add3bc3-d046-4e33-8436-d92e35c12e85/wd:/mnt/batch/tasks/workitems/67279c7f-ca32-4555-9f75-59da8a4c4229/job-1/af214cf5-8bda-4418-8_9add3bc3-d046-4e33-8436-d92e35c12e85/wd,-v,/mnt/batch/tasks/workitems/67279c7f-ca32-4555-9f75-59da8a4c4229/job-1/af214cf5-8bda-4418-8_9add3bc3-d046-4e33-8436-d92e35c12e85/certs:/mnt/batch/tasks/workitems/67279c7f-ca32-4555-9f75-59da8a4c4229/job-1/af214cf5-8bda-4418-8_9add3bc3-d046-4e33-8436-d92e35c12e85/certs\n",
      ">>>   2021/11/04 20:03:33 Running Docker command: docker run --ulimit memlock=9223372036854775807 --ulimit nofile=262144:262144 --cap-add sys_ptrace --name af214cf5-8bda-4418-8cc8-d1dcf4813860 -m 13674m -w /mnt/batch/tasks/shared/LS_root/jobs/hw2_pipeline/azureml/af214cf5-8bda-4418-8cc8-d1dcf4813860/wd --expose 23 --env-file /mnt/batch/tasks/shared/LS_root/jobs/hw2_pipeline/azureml/af214cf5-8bda-4418-8cc8-d1dcf4813860/config/.batchai.envlist --cgroup-parent=/af214cf5-8bda-4418-8cc8-d1dcf4813860/ --shm-size 2g -v /mnt/batch/tasks/startup:/mnt/batch/tasks/startup -v /mnt/batch/tasks/shared/LS_root/mounts:/mnt/batch/tasks/shared/LS_root/mounts:rshared -v /mnt/batch/tasks/shared/LS_root/shared:/mnt/batch/tasks/shared/LS_root/shared -v /mnt/batch/tasks/shared/LS_root/configs:/mnt/batch/tasks/shared/LS_root/configs -v /mnt/batch/tasks/shared/LS_root/jobs/hw2_pipeline/azureml/af214cf5-8bda-4418-8cc8-d1dcf4813860:/mnt/batch/tasks/shared/LS_root/jobs/hw2_pipeline/azureml/af214cf5-8bda-4418-8cc8-d1dcf4813860 -v /mnt/batch/tasks/workitems/67279c7f-ca32-4555-9f75-59da8a4c4229/job-1/af214cf5-8bda-4418-8_9add3bc3-d046-4e33-8436-d92e35c12e85/wd:/mnt/batch/tasks/workitems/67279c7f-ca32-4555-9f75-59da8a4c4229/job-1/af214cf5-8bda-4418-8_9add3bc3-d046-4e33-8436-d92e35c12e85/wd -v /mnt/batch/tasks/workitems/67279c7f-ca32-4555-9f75-59da8a4c4229/job-1/af214cf5-8bda-4418-8_9add3bc3-d046-4e33-8436-d92e35c12e85/certs:/mnt/batch/tasks/workitems/67279c7f-ca32-4555-9f75-59da8a4c4229/job-1/af214cf5-8bda-4418-8_9add3bc3-d046-4e33-8436-d92e35c12e85/certs -d -it --privileged --net=host 96758eaafa3647d4a83e17ccee49d173.azurecr.io/azureml/azureml_74b5d7b1bfec63ece12721fc9d263e9a\n",
      ">>>   2021/11/04 20:03:33 Check if container af214cf5-8bda-4418-8cc8-d1dcf4813860 already exist exited with 0, \n",
      ">>>   \n",
      ">>>   2021/11/04 20:03:33 Check if container af214cf5-8bda-4418-8cc8-d1dcf4813860 already exist exited with 0, \n",
      ">>>   \n",
      ">>>   2021/11/04 20:03:33 Parameters for containerSetup task: useDetonationChamer set to false and sshRequired set to false \n",
      ">>>   2021/11/04 20:03:33 Parameters for containerSetup task: useDetonationChamer set to false and sshRequired set to false \n",
      ">>>   2021/11/04 20:03:33 containerSetup task cmd: [/mnt/batch/tasks/startup/wd/hosttools -task=containerSetup -traceContext=00-0ae1445cbfb117efbbfd80bb82b419d2-aa8db9e3f358b901-01 -sshRequired=false] \n",
      ">>>   2021/11/04 20:03:33 containerSetup task cmd: [/mnt/batch/tasks/startup/wd/hosttools -task=containerSetup -traceContext=00-0ae1445cbfb117efbbfd80bb82b419d2-aa8db9e3f358b901-01 -sshRequired=false] \n",
      ">>>   2021/11/04 20:03:34 Container ssh is not required for job type.\n",
      ">>>   2021/11/04 20:03:34 Starting docker container succeeded.\n",
      ">>>   2021/11/04 20:03:34 Starting docker container succeeded.\n",
      ">>>   2021/11/04 20:03:34 Disk space after starting docker container: 23317MB\n",
      ">>>   2021/11/04 20:03:34 SidecarEnabled:: isDetonationChamber: false, useDockerContainer: true\n",
      ">>>   2021/11/04 20:03:34 SidecarEnabled:: AmlDatasetContextManagerConfig exists: false\n",
      ">>>   2021/11/04 20:03:34 SidecarEnabled:: sidecar not enabled\n",
      ">>>   2021/11/04 20:03:34 Begin execution of runSpecialJobTask\n",
      ">>>   2021/11/04 20:03:34 Creating directory at $AZUREML_LOGDIRECTORY_PATH\n",
      ">>>   2021/11/04 20:03:34 Creating directory /mnt/batch/tasks/shared/LS_root/jobs/hw2_pipeline/azureml/af214cf5-8bda-4418-8cc8-d1dcf4813860/wd/azureml/af214cf5-8bda-4418-8cc8-d1dcf4813860/azureml-logs\n",
      ">>>   2021/11/04 20:03:34 runSpecialJobTask: os.GetEnv constants.StdouterrDir: /mnt/batch/tasks/shared/LS_root/jobs/hw2_pipeline/azureml/af214cf5-8bda-4418-8cc8-d1dcf4813860/wd/azureml/af214cf5-8bda-4418-8cc8-d1dcf4813860/azureml_compute_logs\n",
      ">>>   2021/11/04 20:03:34 runSpecialJobTask: Raw cmd for preparation is passed is: /azureml-envs/azureml_37123770cfa02a1fe00f423fa57ee472/bin/python /mnt/batch/tasks/shared/LS_root/jobs/hw2_pipeline/azureml/af214cf5-8bda-4418-8cc8-d1dcf4813860/mounts/workspaceblobstore/azureml/af214cf5-8bda-4418-8cc8-d1dcf4813860-setup/job_prep.py -i DataStoreCopy:context_managers.DataStores --snapshots '[{\"Id\":\"beb42c74-89f3-40d2-b4d7-f2b6ee525c2f\",\"PathStack\":[\".\"],\"SnapshotEntityId\":null}]'\n",
      ">>>   2021/11/04 20:03:34 runSpecialJobTask: stdout path for preparation is passed is: /mnt/batch/tasks/shared/LS_root/jobs/hw2_pipeline/azureml/af214cf5-8bda-4418-8cc8-d1dcf4813860/wd/azureml/af214cf5-8bda-4418-8cc8-d1dcf4813860/azureml_compute_logs/65_job_prep-tvmps_53bb39c91e5339d39dd46dc46ed72174a017c78327fbfa2f7f38d3d35f9977ea_d.txt\n",
      ">>>   2021/11/04 20:03:34 runSpecialJobTask: stderr path for preparation is passed is: /mnt/batch/tasks/shared/LS_root/jobs/hw2_pipeline/azureml/af214cf5-8bda-4418-8cc8-d1dcf4813860/wd/azureml/af214cf5-8bda-4418-8cc8-d1dcf4813860/azureml_compute_logs/65_job_prep-tvmps_53bb39c91e5339d39dd46dc46ed72174a017c78327fbfa2f7f38d3d35f9977ea_d.txt\n",
      ">>>   2021/11/04 20:03:34 native cmd: export AZUREML_JOB_TASK_ERROR_PATH='/mnt/batch/tasks/workitems/67279c7f-ca32-4555-9f75-59da8a4c4229/job-1/af214cf5-8bda-4418-8_9add3bc3-d046-4e33-8436-d92e35c12e85/wd/runSpecialJobTask_error.json';cd /mnt/batch/tasks/shared/LS_root/jobs/hw2_pipeline/azureml/af214cf5-8bda-4418-8cc8-d1dcf4813860/wd/azureml/af214cf5-8bda-4418-8cc8-d1dcf4813860;/azureml-envs/azureml_37123770cfa02a1fe00f423fa57ee472/bin/python /mnt/batch/tasks/shared/LS_root/jobs/hw2_pipeline/azureml/af214cf5-8bda-4418-8cc8-d1dcf4813860/mounts/workspaceblobstore/azureml/af214cf5-8bda-4418-8cc8-d1dcf4813860-setup/job_prep.py -i DataStoreCopy:context_managers.DataStores --snapshots '[{\"Id\":\"beb42c74-89f3-40d2-b4d7-f2b6ee525c2f\",\"PathStack\":[\".\"],\"SnapshotEntityId\":null}]'\n",
      ">>>   2021/11/04 20:03:34 runSpecialJobTask: commons.GetOsPlatform(): ubuntu\n",
      ">>>   2021/11/04 20:03:34 runSpecialJobTask: Running cmd: /usr/bin/docker exec -e AZUREML_SDK_TRACEPARENT=00-0ae1445cbfb117efbbfd80bb82b419d2-2342de02afddd312-01 -t af214cf5-8bda-4418-8cc8-d1dcf4813860 bash -c if [ -f ~/.bashrc ]; then PS1_back=$PS1; PS1='$'; . ~/.bashrc; PS1=$PS1_back; fi;PATH=$PATH:$AZ_BATCH_NODE_STARTUP_DIR/wd/;export AZUREML_JOB_TASK_ERROR_PATH='/mnt/batch/tasks/workitems/67279c7f-ca32-4555-9f75-59da8a4c4229/job-1/af214cf5-8bda-4418-8_9add3bc3-d046-4e33-8436-d92e35c12e85/wd/runSpecialJobTask_error.json';cd /mnt/batch/tasks/shared/LS_root/jobs/hw2_pipeline/azureml/af214cf5-8bda-4418-8cc8-d1dcf4813860/wd/azureml/af214cf5-8bda-4418-8cc8-d1dcf4813860;/azureml-envs/azureml_37123770cfa02a1fe00f423fa57ee472/bin/python /mnt/batch/tasks/shared/LS_root/jobs/hw2_pipeline/azureml/af214cf5-8bda-4418-8cc8-d1dcf4813860/mounts/workspaceblobstore/azureml/af214cf5-8bda-4418-8cc8-d1dcf4813860-setup/job_prep.py -i DataStoreCopy:context_managers.DataStores --snapshots '[{\"Id\":\"beb42c74-89f3-40d2-b4d7-f2b6ee525c2f\",\"PathStack\":[\".\"],\"SnapshotEntityId\":null}]'\n",
      ">>>   2021/11/04 20:03:36 Attempt 1 of http call to https://centralus.api.azureml.ms/history/v1.0/private/subscriptions/441cf665-f2bb-42da-9467-9dc54a3ae431/resourceGroups/pipeline/providers/Microsoft.MachineLearningServices/workspaces/hw2_pipeline/runs/af214cf5-8bda-4418-8cc8-d1dcf4813860/spans\n",
      ">>>   2021/11/04 20:03:37 containerName:af214cf5-8bda-4418-8cc8-d1dcf4813860\n",
      ">>>   2021/11/04 20:03:37 sidecar containerName:af214cf5-8bda-4418-8cc8-d1dcf4813860\n",
      ">>>   2021/11/04 20:03:37 Docker Version that this nodes use are: 19.03.14+azure\n",
      ">>>   \n",
      ">>>   2021/11/04 20:03:37 The vmsize standard_ds11_v2 is not a GPU VM, skipping get GPU count by running nvidia-smi command.\n",
      ">>>   2021/11/04 20:03:37 The vmsize standard_ds11_v2 is not a GPU VM, skipping get GPU count by running nvidia-smi command.\n",
      ">>>   2021/11/04 20:03:37 sidecar dockerLauncher:docker\n",
      ">>>   2021/11/04 20:03:37 sidecarContainerId:7a9fb636caf9d2f5f51b4b36e275bd3e587535afbd3c378aa100a44a13fb43b6\n",
      ">>>   2021/11/04 20:03:37 Docker Version that this nodes use are: 19.03.14+azure\n",
      ">>>   \n",
      ">>>   2021/11/04 20:03:37 The vmsize standard_ds11_v2 is not a GPU VM, skipping get GPU count by running nvidia-smi command.\n",
      ">>>   2021/11/04 20:03:37 The vmsize standard_ds11_v2 is not a GPU VM, skipping get GPU count by running nvidia-smi command.\n",
      ">>>   2021/11/04 20:03:37 Docker logs for af214cf5-8bda-4418-8cc8-d1dcf4813860\n",
      ">>>   \n",
      ">>>   2021/11/04 20:03:37 runSpecialJobTask: job preparation exited with code 0 and err <nil>\n",
      ">>>   \n",
      ">>>   2021/11/04 20:03:37 runSpecialJobTask: preparation: [2021-11-04T20:03:35.112049] Entering job preparation.\n",
      ">>>   2021/11/04 20:03:37 runSpecialJobTask: preparation: [2021-11-04T20:03:36.689302] Starting job preparation.\n",
      ">>>   2021/11/04 20:03:37 runSpecialJobTask: preparation: [2021-11-04T20:03:36.689333] Extracting the control code.\n",
      ">>>   2021/11/04 20:03:37 runSpecialJobTask: preparation: [2021-11-04T20:03:36.689805] Starting extract_project.\n",
      ">>>   2021/11/04 20:03:37 runSpecialJobTask: preparation: [2021-11-04T20:03:36.689848] Starting to extract zip file.\n",
      ">>>   2021/11/04 20:03:37 runSpecialJobTask: preparation: [2021-11-04T20:03:36.708589] Finished extracting zip file.\n",
      ">>>   2021/11/04 20:03:37 runSpecialJobTask: preparation: [2021-11-04T20:03:36.711606] Using urllib.request Python 3.0 or later\n",
      ">>>   2021/11/04 20:03:37 runSpecialJobTask: preparation: [2021-11-04T20:03:36.711663] Start fetching snapshots.\n",
      ">>>   2021/11/04 20:03:37 runSpecialJobTask: preparation: [2021-11-04T20:03:36.711691] Start fetching snapshot.\n",
      ">>>   2021/11/04 20:03:37 runSpecialJobTask: preparation: [2021-11-04T20:03:36.711706] Retrieving project from snapshot: beb42c74-89f3-40d2-b4d7-f2b6ee525c2f\n",
      ">>>   2021/11/04 20:03:37 runSpecialJobTask: preparation: Starting the daemon thread to refresh tokens in background for process with pid = 44\n",
      ">>>   2021/11/04 20:03:37 runSpecialJobTask: preparation: [2021-11-04T20:03:36.901149] Finished fetching snapshot.\n",
      ">>>   2021/11/04 20:03:37 runSpecialJobTask: preparation: [2021-11-04T20:03:36.901186] Finished fetching snapshots.\n",
      ">>>   2021/11/04 20:03:37 runSpecialJobTask: preparation: [2021-11-04T20:03:36.901196] Finished extract_project.\n",
      ">>>   2021/11/04 20:03:37 runSpecialJobTask: preparation: [2021-11-04T20:03:36.901455] Finished fetching and extracting the control code.\n",
      ">>>   2021/11/04 20:03:37 runSpecialJobTask: preparation: [2021-11-04T20:03:36.905066] downloadDataStore - Download from datastores if requested.\n",
      ">>>   2021/11/04 20:03:37 runSpecialJobTask: preparation: [2021-11-04T20:03:36.906034] Start run_history_prep.\n",
      ">>>   2021/11/04 20:03:37 runSpecialJobTask: preparation: [2021-11-04T20:03:36.918940] Entering context manager injector.\n",
      ">>>   2021/11/04 20:03:37 runSpecialJobTask: preparation: Acquired lockfile /tmp/af214cf5-8bda-4418-8cc8-d1dcf4813860-datastore.lock to downloading input data references\n",
      ">>>   2021/11/04 20:03:37 runSpecialJobTask: preparation: [2021-11-04T20:03:37.327866] downloadDataStore completed\n",
      ">>>   2021/11/04 20:03:37 runSpecialJobTask: preparation: [2021-11-04T20:03:37.330641] Job preparation is complete.\n",
      ">>>   2021/11/04 20:03:37 DockerSideCarContainerLogs:\n",
      ">>>   \n",
      ">>>   2021/11/04 20:03:37 DockerSideCarContainerLogs End\n",
      ">>>   2021/11/04 20:03:37 Execution of runSpecialJobTask completed\n",
      ">>>   2021/11/04 20:03:37 Not exporting to RunHistory as the exporter is either stopped or there is no data.\n",
      ">>>   Stopped: false\n",
      ">>>   OriginalData: 3\n",
      ">>>   FilteredData: 0.\n",
      ">>>   2021/11/04 20:03:37 Process Exiting with Code:  0\n",
      ">>>   2021/11/04 20:03:38 All App Insights Logs was sent successfully or the close timeout of 10 was reached\n",
      ">>>   \n",
      "2021-11-04T20:03:38Z The vmsize standard_ds11_v2 is not a GPU VM, skipping get GPU count by running nvidia-smi command.\n",
      "2021-11-04T20:03:38Z The vmsize standard_ds11_v2 is not a GPU VM, skipping get GPU count by running nvidia-smi command.\n",
      "2021-11-04T20:03:38Z 127.0.0.1 slots=2 max-slots=2\n",
      "2021-11-04T20:03:38Z launching Custom job\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Streaming azureml-logs/75_job_post-tvmps_53bb39c91e5339d39dd46dc46ed72174a017c78327fbfa2f7f38d3d35f9977ea_d.txt\n",
      "===============================================================================================================\n",
      "[2021-11-04T20:03:50.873909] Entering job release\n",
      "[2021-11-04T20:03:51.640429] Starting job release\n",
      "[2021-11-04T20:03:51.641503] Logging experiment finalizing status in history service.\n",
      "Starting the daemon thread to refresh tokens in background for process with pid = 313\n",
      "[2021-11-04T20:03:51.641892] job release stage : upload_datastore starting...\n",
      "[2021-11-04T20:03:51.648920] job release stage : start importing azureml.history._tracking in run_history_release.\n",
      "[2021-11-04T20:03:51.648949] job release stage : execute_job_release starting...\n",
      "[2021-11-04T20:03:51.649717] job release stage : copy_batchai_cached_logs starting...\n",
      "[2021-11-04T20:03:51.649878] job release stage : copy_batchai_cached_logs completed...\n",
      "[2021-11-04T20:03:51.650475] Entering context manager injector.\n",
      "[2021-11-04T20:03:51.664761] job release stage : upload_datastore completed...\n",
      "[2021-11-04T20:03:51.776510] job release stage : send_run_telemetry starting...\n",
      "[2021-11-04T20:03:51.788378] get vm size and vm region successfully.\n",
      "[2021-11-04T20:03:51.794483] get compute meta data successfully.\n",
      "[2021-11-04T20:03:51.819699] job release stage : execute_job_release completed...\n",
      "[2021-11-04T20:03:51.917733] post artifact meta request successfully.\n",
      "[2021-11-04T20:03:51.949493] upload compute record artifact successfully.\n",
      "[2021-11-04T20:03:51.949615] job release stage : send_run_telemetry completed...\n",
      "[2021-11-04T20:03:51.950033] Job release is complete\n"
     ]
    }
   ],
   "source": [
    "from azureml.core import Experiment\n",
    "from azureml.pipeline.core import Pipeline\n",
    "from azureml.widgets import RunDetails\n",
    "\n",
    "# Construct the pipeline\n",
    "pipeline_steps = [train_step, register_step]\n",
    "pipeline = Pipeline(workspace=ws, steps=pipeline_steps)\n",
    "print(\"Pipeline is built.\")\n",
    "\n",
    "# Create an experiment and run the pipeline\n",
    "experiment = Experiment(workspace=ws, name = 'churn-pipeline')\n",
    "pipeline_run = experiment.submit(pipeline, regenerate_outputs=True)\n",
    "print(\"Pipeline submitted for execution.\")\n",
    "RunDetails(pipeline_run).show()\n",
    "pipeline_run.wait_for_completion(show_output=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
