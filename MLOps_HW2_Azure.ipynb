{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0e4a9f51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SDK Version: 1.35.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import azureml.core\n",
    "from azureml.core import Workspace, Experiment, Datastore\n",
    "from azureml.widgets import RunDetails\n",
    "\n",
    "from azureml.core import Dataset\n",
    "\n",
    "from azureml.pipeline.core import Pipeline, PipelineData, PipelineRun, StepRun, PortDataReference\n",
    "from azureml.pipeline.steps import PythonScriptStep\n",
    "\n",
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "from azureml.core.runconfig import RunConfiguration\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "\n",
    "from azureml.core.model import Model\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print('SDK Version:', azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f275feb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Workspace name: hw2_pipeline\n",
      "Azure region: centralus\n",
      "Subscription id: 441cf665-f2bb-42da-9467-9dc54a3ae431\n",
      "Resource group: pipeline\n"
     ]
    }
   ],
   "source": [
    "# load workspace\n",
    "ws = Workspace.from_config()\n",
    "print('Workspace name: ' + ws.name, \n",
    "      'Azure region: ' + ws.location, \n",
    "      'Subscription id: ' + ws.subscription_id, \n",
    "      'Resource group: ' + ws.resource_group, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "247719b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "azureml_globaldatasets - Default = False\n",
      "workspaceblobstore - Default = True\n",
      "workspacefilestore - Default = False\n",
      "workspaceworkingdirectory - Default = False\n",
      "workspaceartifactstore - Default = False\n"
     ]
    }
   ],
   "source": [
    "# Get the default datastore\n",
    "default_ds = ws.get_default_datastore()\n",
    "\n",
    "# Enumerate all datastores, indicating which is the default\n",
    "for ds_name in ws.datastores:\n",
    "    print(ds_name, \"- Default =\", ds_name == default_ds.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f751eaab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already registered.\n"
     ]
    }
   ],
   "source": [
    "if 'churn dataset' not in ws.datasets:\n",
    "    default_ds.upload_files(files=['/Users/junruiwan/Desktop/new_data.csv'], # Upload the diabetes csv files in /data\n",
    "                        target_path='churn-data/', # Put it in a folder path in the datastore\n",
    "                        overwrite=True, # Replace existing files of the same name\n",
    "                        show_progress=True)\n",
    "\n",
    "    #Create a tabular dataset from the path on the datastore (this may take a short while)\n",
    "    tab_data_set = Dataset.Tabular.from_delimited_files(path=(default_ds, 'churn-data/new_data.csv'))\n",
    "\n",
    "    # Register the tabular dataset\n",
    "    try:\n",
    "        tab_data_set = tab_data_set.register(workspace=ws, \n",
    "                                name='churn dataset',\n",
    "                                description='churn data',\n",
    "                                tags = {'format':'CSV'},\n",
    "                                create_new_version=True)\n",
    "        print('Dataset registered.')\n",
    "    except Exception as ex:\n",
    "        print(ex)\n",
    "else:\n",
    "    print('Dataset already registered.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "33c29343",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "churn_pipeline\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# Create a folder for the pipeline step files\n",
    "experiment_folder = 'churn_pipeline'\n",
    "os.makedirs(experiment_folder, exist_ok=True)\n",
    "\n",
    "print(experiment_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9d2bcda1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing churn_pipeline/prep_churn.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $experiment_folder/prep_churn.py\n",
    "# Import libraries\n",
    "import os\n",
    "import argparse\n",
    "import pandas as pd\n",
    "from azureml.core import Run\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Get parameters\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--input-data\", type=str, dest='raw_dataset_id', help='raw dataset')\n",
    "parser.add_argument('--prepped-data', type=str, dest='prepped_data', default='prepped_data', help='Folder for results')\n",
    "args = parser.parse_args()\n",
    "save_folder = args.prepped_data\n",
    "\n",
    "# Get the experiment run context\n",
    "run = Run.get_context()\n",
    "\n",
    "# load the data (passed as an input dataset)\n",
    "print(\"Loading Data...\")\n",
    "churn = run.input_datasets['raw_data'].to_pandas_dataframe()\n",
    "\n",
    "# Log raw row count\n",
    "row_count = (len(churn))\n",
    "run.log('raw_rows', row_count)\n",
    "\n",
    "# data cleaning\n",
    "churn = churn.dropna()\n",
    "\n",
    "# Log processed rows\n",
    "row_count = (len(churn))\n",
    "run.log('processed_rows', row_count)\n",
    "\n",
    "# Save the prepped data\n",
    "print(\"Saving Data...\")\n",
    "os.makedirs(save_folder, exist_ok=True)\n",
    "save_path = os.path.join(save_folder,'data.csv')\n",
    "churn.to_csv(save_path, index=False, header=True)\n",
    "\n",
    "# End the run\n",
    "run.complete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d7e42c21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing churn_pipeline/train_churn.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $experiment_folder/train_churn.py\n",
    "# Import libraries\n",
    "from azureml.core import Run, Model\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get parameters\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--training-folder\", type=str, dest='training_folder', help='training data folder')\n",
    "args = parser.parse_args()\n",
    "training_folder = args.training_folder\n",
    "\n",
    "# Get the experiment run context\n",
    "run = Run.get_context()\n",
    "\n",
    "# load the prepared data file in the training folder\n",
    "print(\"Loading Data...\")\n",
    "file_path = os.path.join(training_folder,'data.csv')\n",
    "churn = pd.read_csv(file_path)\n",
    "\n",
    "X = churn[churn.columns.difference(['Attrition_Yes'])].values\n",
    "y = churn['Attrition_Yes'].values\n",
    "\n",
    "# Split data into training set and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=0)\n",
    "\n",
    "# Scale the data, using standardization\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "# Train the random forest model\n",
    "print('Training a random forest model...')\n",
    "model = RandomForestClassifier().fit(X_train, y_train)\n",
    "\n",
    "# calculate accuracy\n",
    "y_hat = model.predict(X_test)\n",
    "acc = np.average(y_hat == y_test)\n",
    "print('Accuracy:', acc)\n",
    "run.log('Accuracy', np.float(acc))\n",
    "\n",
    "# calculate AUC\n",
    "y_scores = model.predict_proba(X_test)\n",
    "auc = roc_auc_score(y_test,y_scores[:,1])\n",
    "print('AUC: ' + str(auc))\n",
    "run.log('AUC', np.float(auc))\n",
    "\n",
    "# plot ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_scores[:,1])\n",
    "fig = plt.figure(figsize=(6, 4))\n",
    "# Plot the diagonal 50% line\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "# Plot the FPR and TPR achieved by our model\n",
    "plt.plot(fpr, tpr)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "run.log_image(name = \"ROC\", plot = fig)\n",
    "plt.show()\n",
    "\n",
    "# Save the trained model in the outputs folder\n",
    "print(\"Saving model...\")\n",
    "os.makedirs('outputs', exist_ok=True)\n",
    "model_file = os.path.join('outputs', 'randomforest_model.pkl')\n",
    "joblib.dump(value=model, filename=model_file)\n",
    "\n",
    "# Register the model\n",
    "print('Registering model...')\n",
    "Model.register(workspace=run.experiment.workspace,\n",
    "               model_path = model_file,\n",
    "               model_name = 'randomforest_model',\n",
    "               tags={'Training context':'Pipeline'},\n",
    "               properties={'AUC': np.float(auc), 'Accuracy': np.float(acc)})\n",
    "\n",
    "\n",
    "run.complete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f20b5c0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing cluster, use it.\n"
     ]
    }
   ],
   "source": [
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "cluster_name = \"mlop-test\"\n",
    "\n",
    "try:\n",
    "    # Check for existing compute target\n",
    "    pipeline_cluster = ComputeTarget(workspace=ws, name=cluster_name)\n",
    "    print('Found existing cluster, use it.')\n",
    "except ComputeTargetException:\n",
    "    # If it doesn't already exist, create it\n",
    "    try:\n",
    "        compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_DS11_V2', max_nodes=2)\n",
    "        pipeline_cluster = ComputeTarget.create(ws, cluster_name, compute_config)\n",
    "        pipeline_cluster.wait_for_completion(show_output=True)\n",
    "    except Exception as ex:\n",
    "        print(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "993db66e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run configuration created.\n"
     ]
    }
   ],
   "source": [
    "from azureml.core import Environment\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "from azureml.core.runconfig import RunConfiguration\n",
    "\n",
    "# Create a Python environment for the experiment\n",
    "churn_env = Environment(\"churn-pipeline-env\")\n",
    "\n",
    "# Create a set of package dependencies\n",
    "churn_packages = CondaDependencies.create(conda_packages=['scikit-learn','ipykernel','matplotlib','pandas','pip'],\n",
    "                                             pip_packages=['azureml-defaults','azureml-dataprep[pandas]','pyarrow'])\n",
    "\n",
    "# Add the dependencies to the environment\n",
    "churn_env.python.conda_dependencies = churn_packages\n",
    "\n",
    "# Register the environment \n",
    "churn_env.register(workspace=ws)\n",
    "registered_env = Environment.get(ws, 'churn-pipeline-env')\n",
    "\n",
    "# Create a new runconfig object for the pipeline\n",
    "pipeline_run_config = RunConfiguration()\n",
    "\n",
    "# Use the compute you created above. \n",
    "pipeline_run_config.target = pipeline_cluster\n",
    "\n",
    "# Assign the environment to the run configuration\n",
    "pipeline_run_config.environment = registered_env\n",
    "\n",
    "print (\"Run configuration created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6ed9a810",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline steps defined\n"
     ]
    }
   ],
   "source": [
    "from azureml.pipeline.core import PipelineData\n",
    "from azureml.pipeline.steps import PythonScriptStep\n",
    "\n",
    "# Get the training dataset\n",
    "churn_ds = ws.datasets.get(\"churn dataset\")\n",
    "\n",
    "# Create a PipelineData (temporary Data Reference) for the model folder\n",
    "prepped_data_folder = PipelineData(\"prepped_data_folder\", datastore=ws.get_default_datastore())\n",
    "\n",
    "# Step 1, Run the data prep script\n",
    "train_step = PythonScriptStep(name = \"Prepare Data\",\n",
    "                                source_directory = experiment_folder,\n",
    "                                script_name = \"prep_churn.py\",\n",
    "                                arguments = ['--input-data', churn_ds.as_named_input('raw_data'),\n",
    "                                             '--prepped-data', prepped_data_folder],\n",
    "                                outputs=[prepped_data_folder],\n",
    "                                compute_target = pipeline_cluster,\n",
    "                                runconfig = pipeline_run_config,\n",
    "                                allow_reuse = True)\n",
    "\n",
    "# Step 2, run the training script\n",
    "register_step = PythonScriptStep(name = \"Train and Register Model\",\n",
    "                                source_directory = experiment_folder,\n",
    "                                script_name = \"train_churn.py\",\n",
    "                                arguments = ['--training-folder', prepped_data_folder],\n",
    "                                inputs=[prepped_data_folder],\n",
    "                                compute_target = pipeline_cluster,\n",
    "                                runconfig = pipeline_run_config,\n",
    "                                allow_reuse = True)\n",
    "\n",
    "print(\"Pipeline steps defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b9fbc2fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline is built.\n",
      "Created step Prepare Data [d23f3212][d8c28e41-2897-461a-994b-0610a70b869a], (This step will run and generate new outputs)Created step Train and Register Model [3bbf5017][6728cbd5-50bd-483c-8afe-670dfa5afc27], (This step will run and generate new outputs)\n",
      "\n",
      "Submitted PipelineRun 2f4f914e-7841-416d-8404-90e36c983bc4\n",
      "Link to Azure Machine Learning Portal: https://ml.azure.com/runs/2f4f914e-7841-416d-8404-90e36c983bc4?wsid=/subscriptions/441cf665-f2bb-42da-9467-9dc54a3ae431/resourcegroups/pipeline/workspaces/hw2_pipeline&tid=6d7adc86-8446-4cb9-9972-1477c831ff28\n",
      "Pipeline submitted for execution.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ea92265913745c283589654edc217da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "_PipelineWidget(widget_settings={'childWidgetDisplay': 'popup', 'send_telemetry': False, 'log_level': 'INFO', …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/aml.mini.widget.v1": "{\"loading\": true}"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PipelineRunId: 2f4f914e-7841-416d-8404-90e36c983bc4\n",
      "Link to Azure Machine Learning Portal: https://ml.azure.com/runs/2f4f914e-7841-416d-8404-90e36c983bc4?wsid=/subscriptions/441cf665-f2bb-42da-9467-9dc54a3ae431/resourcegroups/pipeline/workspaces/hw2_pipeline&tid=6d7adc86-8446-4cb9-9972-1477c831ff28\n",
      "PipelineRun Status: NotStarted\n",
      "PipelineRun Status: Running\n",
      "\n",
      "\n",
      "StepRunId: c7abcd3e-3aee-4283-8c53-f6c5f9204977\n",
      "Link to Azure Machine Learning Portal: https://ml.azure.com/runs/c7abcd3e-3aee-4283-8c53-f6c5f9204977?wsid=/subscriptions/441cf665-f2bb-42da-9467-9dc54a3ae431/resourcegroups/pipeline/workspaces/hw2_pipeline&tid=6d7adc86-8446-4cb9-9972-1477c831ff28\n",
      "StepRun( Prepare Data ) Status: Running\n",
      "\n",
      "Streaming azureml-logs/55_azureml-execution-tvmps_53bb39c91e5339d39dd46dc46ed72174a017c78327fbfa2f7f38d3d35f9977ea_d.txt\n",
      "========================================================================================================================\n",
      "2021-11-04T20:12:41Z Running following command: /bin/bash -c sudo blobfuse /mnt/batch/tasks/shared/LS_root/jobs/hw2_pipeline/azureml/c7abcd3e-3aee-4283-8c53-f6c5f9204977/mounts/workspaceblobstore --tmp-path=/mnt/batch/tasks/shared/LS_root/jobs/hw2_pipeline/azureml/c7abcd3e-3aee-4283-8c53-f6c5f9204977/caches/workspaceblobstore --file-cache-timeout-in-seconds=1000000 --cache-size-mb=21862 -o nonempty -o allow_other --config-file=/mnt/batch/tasks/shared/LS_root/jobs/hw2_pipeline/azureml/c7abcd3e-3aee-4283-8c53-f6c5f9204977/configs/workspaceblobstore.cfg --log-level=LOG_WARNING\n",
      "2021-11-04T20:12:41Z Successfully mounted a/an Blobfuse File System at /mnt/batch/tasks/shared/LS_root/jobs/hw2_pipeline/azureml/c7abcd3e-3aee-4283-8c53-f6c5f9204977/mounts/workspaceblobstore\n",
      "2021-11-04T20:12:42Z The vmsize standard_ds11_v2 is not a GPU VM, skipping get GPU count by running nvidia-smi command.\n",
      "2021-11-04T20:12:42Z Starting output-watcher...\n",
      "2021-11-04T20:12:42Z IsDedicatedCompute == True, won't poll for Low Pri Preemption\n",
      "2021-11-04T20:12:42Z Executing 'Copy ACR Details file' on 10.0.0.4\n",
      "2021-11-04T20:12:42Z Copy ACR Details file succeeded on 10.0.0.4. Output: \n",
      ">>>   \n",
      ">>>   \n",
      "Login Succeeded\n",
      "Using default tag: latest\n",
      "latest: Pulling from azureml/azureml_74b5d7b1bfec63ece12721fc9d263e9a\n",
      "Digest: sha256:b38056d72f7574d4ce9412e84dedd30e2f5025b046ecc28854f5dafcef12c78b\n",
      "Status: Image is up to date for 96758eaafa3647d4a83e17ccee49d173.azurecr.io/azureml/azureml_74b5d7b1bfec63ece12721fc9d263e9a:latest\n",
      "96758eaafa3647d4a83e17ccee49d173.azurecr.io/azureml/azureml_74b5d7b1bfec63ece12721fc9d263e9a:latest\n",
      "2021-11-04T20:12:42Z The vmsize standard_ds11_v2 is not a GPU VM, skipping get GPU count by running nvidia-smi command.\n",
      "2021-11-04T20:12:42Z Check if container c7abcd3e-3aee-4283-8c53-f6c5f9204977 already exist exited with 0, \n",
      "\n",
      "06d23bbc2b339736746557260e163ce7a8a928e60f1da7a708b62577116ffd98\n",
      "2021-11-04T20:12:42Z Parameters for containerSetup task: useDetonationChamer set to false and sshRequired set to false \n",
      "2021-11-04T20:12:42Z containerSetup task cmd: [/mnt/batch/tasks/startup/wd/hosttools -task=containerSetup -traceContext=00-00eeb8c5c95823e4686eb3d63765f7a7-460f384b6b344b0f-01 -sshRequired=false] \n",
      "2021/11/04 20:12:42 Got JobInfoJson from env\n",
      "2021/11/04 20:12:42 Starting App Insight Logger for task:  containerSetup\n",
      "2021/11/04 20:12:42 Version: 3.0.01755.0003 Branch: .SourceBranch Commit: 66828d8\n",
      "2021/11/04 20:12:42 Entered ContainerSetupTask - Preparing infiniband\n",
      "2021/11/04 20:12:42 Starting infiniband setup\n",
      "2021/11/04 20:12:43 Python Version found is Python 3.6.2 :: Anaconda, Inc.\n",
      "\n",
      "2021/11/04 20:12:43 Returning Python Version as 3.6\n",
      "2021/11/04 20:12:43 VMSize: standard_ds11_v2, Host: runtime-gen1-ubuntu18, Container: ubuntu-18.04\n",
      "2021/11/04 20:12:43 VMSize: standard_ds11_v2, Host: runtime-gen1-ubuntu18, Container: ubuntu-18.04\n",
      "2021-11-04T20:12:43Z VMSize: standard_ds11_v2, Host: runtime-gen1-ubuntu18, Container: ubuntu-18.04\n",
      "2021/11/04 20:12:43 /dev/infiniband/uverbs0 found (implying presence of InfiniBand)?: false\n",
      "2021-11-04T20:12:43Z Not setting up Infiniband in Container\n",
      "2021/11/04 20:12:43 Not setting up Infiniband in Container\n",
      "2021/11/04 20:12:43 Not setting up Infiniband in Container\n",
      "2021/11/04 20:12:43 Python Version found is Python 3.6.2 :: Anaconda, Inc.\n",
      "\n",
      "2021/11/04 20:12:43 Returning Python Version as 3.6\n",
      "2021/11/04 20:12:43 sshd inside container not required for job, skipping setup.\n",
      "2021/11/04 20:12:43 All App Insights Logs was sent successfully or the close timeout of 10 was reached\n",
      "2021/11/04 20:12:43 App Insight Client has already been closed\n",
      "2021/11/04 20:12:43 Not exporting to RunHistory as the exporter is either stopped or there is no data.\n",
      "Stopped: false\n",
      "OriginalData: 1\n",
      "FilteredData: 0.\n",
      "2021-11-04T20:12:43Z Starting docker container succeeded.\n",
      "2021-11-04T20:12:48Z The vmsize standard_ds11_v2 is not a GPU VM, skipping get GPU count by running nvidia-smi command.\n",
      "2021-11-04T20:12:48Z The vmsize standard_ds11_v2 is not a GPU VM, skipping get GPU count by running nvidia-smi command.\n",
      "2021-11-04T20:12:49Z Job environment preparation succeeded on 10.0.0.4. Output: \n",
      ">>>   2021/11/04 20:12:41 Got JobInfoJson from env\n",
      ">>>   2021/11/04 20:12:41 Starting App Insight Logger for task:  prepareJobEnvironment\n",
      ">>>   2021/11/04 20:12:41 Version: 3.0.01755.0003 Branch: .SourceBranch Commit: 66828d8\n",
      ">>>   2021/11/04 20:12:41 Got JobInfoJson from env\n",
      ">>>   2021/11/04 20:12:41 runtime.GOOS linux\n",
      ">>>   2021/11/04 20:12:41 Checking if '/tmp' exists\n",
      ">>>   2021/11/04 20:12:41 Reading dyanamic configs\n",
      ">>>   2021/11/04 20:12:41 Container sas url: https://baiscriptsdm1prod.blob.core.windows.net/aihosttools?sv=2018-03-28&sr=c&si=aihosttoolspolicy&sig=0dYPjOcglC72KKRE2ILzFCKnPug7ECc2SIRWA3udLW0%3D\n",
      ">>>   2021/11/04 20:12:41 Starting Azsecpack installation on machine: e452e77d3e2f44698f1de6e630291a79000000#6d7adc86-8446-4cb9-9972-1477c831ff28#441cf665-f2bb-42da-9467-9dc54a3ae431#pipeline#hw2_pipeline#mlop-test#tvmps_53bb39c91e5339d39dd46dc46ed72174a017c78327fbfa2f7f38d3d35f9977ea_d\n",
      ">>>   2021/11/04 20:12:41 Failed to read from file /mnt/batch/tasks/startup/wd/az_resource/azsecpack.variables, open /mnt/batch/tasks/startup/wd/az_resource/azsecpack.variables: no such file or directory\n",
      ">>>   2021/11/04 20:12:41 Azsecpack installation directory: /mnt/batch/tasks/startup/wd/az_resource, Is Azsecpack installer on host: true. Is Azsecpack installation enabled: false,\n",
      ">>>   2021/11/04 20:12:41 Is Azsecpack enabled: false, GetDisableVsatlsscan: true\n",
      ">>>   2021/11/04 20:12:41 Turning off azsecpack, if it is already running\n",
      ">>>   2021/11/04 20:12:41 Start deleting Azsecpack installation cronjob...\n",
      ">>>   2021/11/04 20:12:41 Start checking if Azsecpack is running...\n",
      ">>>   2021/11/04 20:12:41 Azsecpack is not running. No need to stop Azsecpack processes.\n",
      ">>>   2021/11/04 20:12:41 bypass systemd resolved\n",
      ">>>   2021/11/04 20:12:41 Cluster Subscription Id: 441cf665-f2bb-42da-9467-9dc54a3ae431\n",
      ">>>   2021/11/04 20:12:41 Cluster Workspace Name: hw2_pipeline\n",
      ">>>   2021/11/04 20:12:41 Cluster Name: mlop-test\n",
      ">>>   2021/11/04 20:12:41 VMsize: standard_ds11_v2\n",
      ">>>   2021/11/04 20:12:41 GPU Count: 0\n",
      ">>>   2021/11/04 20:12:41 Job: AZ_BATCHAI_JOB_NAME does not turn on the DetonationChamber\n",
      ">>>   2021/11/04 20:12:41 The vmsize standard_ds11_v2 is not a GPU VM, skipping get GPU count by running nvidia-smi command.\n",
      ">>>   2021/11/04 20:12:41 The vmsize standard_ds11_v2 is not a GPU VM, skipping get GPU count by running nvidia-smi command.\n",
      ">>>   2021/11/04 20:12:41 Get GPU count failed with err: The vmsize standard_ds11_v2 is not a GPU VM, skipping get GPU count by running nvidia-smi command., \n",
      ">>>   2021/11/04 20:12:41 AMLComputeXDSEndpoint:  https://centralus.cert.api.azureml.ms/xdsbatchai\n",
      ">>>   2021/11/04 20:12:41 AMLComputeXDSApiVersion:  2018-02-01\n",
      ">>>   2021/11/04 20:12:41 Creating directory /mnt/batch/tasks/shared/LS_root/jobs/hw2_pipeline/azureml/c7abcd3e-3aee-4283-8c53-f6c5f9204977/config\n",
      ">>>   2021/11/04 20:12:41 This is not a aml-workstation (compute instance), current offer type: amlcompute. Starting identity responder as part of prepareJobEnvironment.\n",
      ">>>   2021/11/04 20:12:41 Starting identity responder.\n",
      ">>>   2021/11/04 20:12:41 Starting identity responder.\n",
      ">>>   2021/11/04 20:12:41 Logfile used for identity responder: /mnt/batch/tasks/workitems/67279c7f-ca32-4555-9f75-59da8a4c4229/job-1/c7abcd3e-3aee-4283-8_af76dac5-a51e-41b2-b62b-bba1cc4822d7/IdentityResponderLog-tvmps_53bb39c91e5339d39dd46dc46ed72174a017c78327fbfa2f7f38d3d35f9977ea_d.txt\n",
      ">>>   2021/11/04 20:12:41 Logfile used for identity responder: /mnt/batch/tasks/workitems/67279c7f-ca32-4555-9f75-59da8a4c4229/job-1/c7abcd3e-3aee-4283-8_af76dac5-a51e-41b2-b62b-bba1cc4822d7/IdentityResponderLog-tvmps_53bb39c91e5339d39dd46dc46ed72174a017c78327fbfa2f7f38d3d35f9977ea_d.txt\n",
      ">>>   2021/11/04 20:12:41 Started Identity Responder for job.\n",
      ">>>   2021/11/04 20:12:41 Started Identity Responder for job.\n",
      ">>>   2021/11/04 20:12:41 Creating directory /mnt/batch/tasks/shared/LS_root/jobs/hw2_pipeline/azureml/c7abcd3e-3aee-4283-8c53-f6c5f9204977/wd\n",
      ">>>   2021/11/04 20:12:41 Creating directory /mnt/batch/tasks/shared/LS_root/jobs/hw2_pipeline/azureml/c7abcd3e-3aee-4283-8c53-f6c5f9204977/shared\n",
      ">>>   2021/11/04 20:12:41 WorkingDirPath is specified. Setting env AZ_BATCHAI_JOB_WORK_DIR=$AZ_BATCHAI_JOB_TEMP/azureml/c7abcd3e-3aee-4283-8c53-f6c5f9204977\n",
      ">>>   2021/11/04 20:12:41 From the policy service, the filtering patterns is: , data store is \n",
      ">>>   2021/11/04 20:12:41 Mounting job level file systems\n",
      ">>>   2021/11/04 20:12:41 Creating directory /mnt/batch/tasks/shared/LS_root/jobs/hw2_pipeline/azureml/c7abcd3e-3aee-4283-8c53-f6c5f9204977/mounts\n",
      ">>>   2021/11/04 20:12:41 Attempting to read datastore credentials file: /mnt/batch/tasks/shared/LS_root/jobs/hw2_pipeline/azureml/c7abcd3e-3aee-4283-8c53-f6c5f9204977/config/.amlcompute.datastorecredentials\n",
      ">>>   2021/11/04 20:12:41 Datastore credentials file not found, skipping.\n",
      ">>>   2021/11/04 20:12:41 Attempting to read runtime sas tokens file: /mnt/batch/tasks/shared/LS_root/jobs/hw2_pipeline/azureml/c7abcd3e-3aee-4283-8c53-f6c5f9204977/config/.master.runtimesastokens\n",
      ">>>   2021/11/04 20:12:41 Runtime sas tokens file not found, skipping.\n",
      ">>>   2021/11/04 20:12:41 NFS mount is not enabled\n",
      ">>>   2021/11/04 20:12:41 No Azure File Shares configured\n",
      ">>>   2021/11/04 20:12:41 Mounting blob file systems\n",
      ">>>   2021/11/04 20:12:41 Blobfuse runtime version 1.3.6\n",
      ">>>   2021/11/04 20:12:41 Mounting azureml-blobstore-96758eaa-fa36-47d4-a83e-17ccee49d173 container from hw2pipeline7446083280 account at /mnt/batch/tasks/shared/LS_root/jobs/hw2_pipeline/azureml/c7abcd3e-3aee-4283-8c53-f6c5f9204977/mounts/workspaceblobstore\n",
      ">>>   2021/11/04 20:12:41 Using Compute Identity to authenticate Blobfuse: false.\n",
      ">>>   2021/11/04 20:12:41 Using Compute Identity to authenticate Blobfuse: false.\n",
      ">>>   2021/11/04 20:12:41 Blobfuse cache size set to 21862 MB.\n",
      ">>>   2021/11/04 20:12:41 Running following command: /bin/bash -c sudo blobfuse /mnt/batch/tasks/shared/LS_root/jobs/hw2_pipeline/azureml/c7abcd3e-3aee-4283-8c53-f6c5f9204977/mounts/workspaceblobstore --tmp-path=/mnt/batch/tasks/shared/LS_root/jobs/hw2_pipeline/azureml/c7abcd3e-3aee-4283-8c53-f6c5f9204977/caches/workspaceblobstore --file-cache-timeout-in-seconds=1000000 --cache-size-mb=21862 -o nonempty -o allow_other --config-file=/mnt/batch/tasks/shared/LS_root/jobs/hw2_pipeline/azureml/c7abcd3e-3aee-4283-8c53-f6c5f9204977/configs/workspaceblobstore.cfg --log-level=LOG_WARNING\n",
      ">>>   2021/11/04 20:12:41 Successfully mounted a/an Blobfuse File System at /mnt/batch/tasks/shared/LS_root/jobs/hw2_pipeline/azureml/c7abcd3e-3aee-4283-8c53-f6c5f9204977/mounts/workspaceblobstore\n",
      ">>>   2021/11/04 20:12:41 Waiting for blobfs to be mounted at /mnt/batch/tasks/shared/LS_root/jobs/hw2_pipeline/azureml/c7abcd3e-3aee-4283-8c53-f6c5f9204977/mounts/workspaceblobstore\n",
      ">>>   2021/11/04 20:12:41 Successfully mounted azureml-blobstore-96758eaa-fa36-47d4-a83e-17ccee49d173 container from hw2pipeline7446083280 account at /mnt/batch/tasks/shared/LS_root/jobs/hw2_pipeline/azureml/c7abcd3e-3aee-4283-8c53-f6c5f9204977/mounts/workspaceblobstore\n",
      ">>>   2021/11/04 20:12:42 Created run_id directory: /mnt/batch/tasks/shared/LS_root/jobs/hw2_pipeline/azureml/c7abcd3e-3aee-4283-8c53-f6c5f9204977/mounts/workspaceblobstore/azureml/c7abcd3e-3aee-4283-8c53-f6c5f9204977\n",
      ">>>   2021/11/04 20:12:42 No unmanaged file systems configured\n",
      ">>>   2021/11/04 20:12:42 The vmsize standard_ds11_v2 is not a GPU VM, skipping get GPU count by running nvidia-smi command.\n",
      ">>>   2021/11/04 20:12:42 The vmsize standard_ds11_v2 is not a GPU VM, skipping get GPU count by running nvidia-smi command.\n",
      ">>>   2021/11/04 20:12:42 WorkingDirPath is specified. Setting env AZ_BATCHAI_JOB_WORK_DIR=$AZ_BATCHAI_JOB_TEMP/azureml/c7abcd3e-3aee-4283-8c53-f6c5f9204977\n",
      ">>>   2021/11/04 20:12:42 From the policy service, the filtering patterns is: , data store is \n",
      ">>>   2021/11/04 20:12:42 Creating working directory: /mnt/batch/tasks/shared/LS_root/jobs/hw2_pipeline/azureml/c7abcd3e-3aee-4283-8c53-f6c5f9204977/wd/azureml/c7abcd3e-3aee-4283-8c53-f6c5f9204977\n",
      ">>>   2021/11/04 20:12:42 Creating directory /mnt/batch/tasks/shared/LS_root/jobs/hw2_pipeline/azureml/c7abcd3e-3aee-4283-8c53-f6c5f9204977/wd/azureml/c7abcd3e-3aee-4283-8c53-f6c5f9204977\n",
      ">>>   2021/11/04 20:12:42 Changing permissions for all existing files under directory: /mnt/batch/tasks/shared/LS_root/jobs/hw2_pipeline/azureml/c7abcd3e-3aee-4283-8c53-f6c5f9204977/wd/azureml/c7abcd3e-3aee-4283-8c53-f6c5f9204977\n",
      ">>>   2021/11/04 20:12:42 Change mode to 777 for dir /mnt/batch/tasks/shared/LS_root/jobs/hw2_pipeline/azureml/c7abcd3e-3aee-4283-8c53-f6c5f9204977/wd/azureml/c7abcd3e-3aee-4283-8c53-f6c5f9204977\n",
      ">>>   2021/11/04 20:12:42 Change mode to 777 for dir /mnt/batch/tasks/shared/LS_root/jobs/hw2_pipeline/azureml/c7abcd3e-3aee-4283-8c53-f6c5f9204977/wd/azureml/c7abcd3e-3aee-4283-8c53-f6c5f9204977/azureml_compute_logs\n",
      ">>>   2021/11/04 20:12:42 Change mode to 777 for dir /mnt/batch/tasks/shared/LS_root/jobs/hw2_pipeline/azureml/c7abcd3e-3aee-4283-8c53-f6c5f9204977/wd/azureml/c7abcd3e-3aee-4283-8c53-f6c5f9204977/azureml_compute_logs/tvmps_53bb39c91e5339d39dd46dc46ed72174a017c78327fbfa2f7f38d3d35f9977ea_d\n",
      ">>>   2021/11/04 20:12:42 Change mode to 666 for file /mnt/batch/tasks/shared/LS_root/jobs/hw2_pipeline/azureml/c7abcd3e-3aee-4283-8c53-f6c5f9204977/wd/azureml/c7abcd3e-3aee-4283-8c53-f6c5f9204977/azureml_compute_logs/tvmps_53bb39c91e5339d39dd46dc46ed72174a017c78327fbfa2f7f38d3d35f9977ea_d/55_azureml-execution-tvmps_53bb39c91e5339d39dd46dc46ed72174a017c78327fbfa2f7f38d3d35f9977ea_d.txt\n",
      ">>>   2021/11/04 20:12:42 Set default ACL for files under directory by running: /usr/bin/setfacl -m default:g::rwx -m default:o::rwx /mnt/batch/tasks/shared/LS_root/jobs/hw2_pipeline/azureml/c7abcd3e-3aee-4283-8c53-f6c5f9204977/wd/azureml/c7abcd3e-3aee-4283-8c53-f6c5f9204977\n",
      ">>>   2021/11/04 20:12:42 Creating directory /mnt/batch/tasks/shared/LS_root/jobs/hw2_pipeline/azureml/c7abcd3e-3aee-4283-8c53-f6c5f9204977/wd/azureml/c7abcd3e-3aee-4283-8c53-f6c5f9204977/azureml_compute_logs\n",
      ">>>   2021/11/04 20:12:42 Changing permissions for all existing files under directory: /mnt/batch/tasks/shared/LS_root/jobs/hw2_pipeline/azureml/c7abcd3e-3aee-4283-8c53-f6c5f9204977/wd/azureml/c7abcd3e-3aee-4283-8c53-f6c5f9204977/azureml_compute_logs\n",
      ">>>   2021/11/04 20:12:42 Change mode to 777 for dir /mnt/batch/tasks/shared/LS_root/jobs/hw2_pipeline/azureml/c7abcd3e-3aee-4283-8c53-f6c5f9204977/wd/azureml/c7abcd3e-3aee-4283-8c53-f6c5f9204977/azureml_compute_logs\n",
      ">>>   2021/11/04 20:12:42 Change mode to 777 for dir /mnt/batch/tasks/shared/LS_root/jobs/hw2_pipeline/azureml/c7abcd3e-3aee-4283-8c53-f6c5f9204977/wd/azureml/c7abcd3e-3aee-4283-8c53-f6c5f9204977/azureml_compute_logs/tvmps_53bb39c91e5339d39dd46dc46ed72174a017c78327fbfa2f7f38d3d35f9977ea_d\n",
      ">>>   2021/11/04 20:12:42 Change mode to 666 for file /mnt/batch/tasks/shared/LS_root/jobs/hw2_pipeline/azureml/c7abcd3e-3aee-4283-8c53-f6c5f9204977/wd/azureml/c7abcd3e-3aee-4283-8c53-f6c5f9204977/azureml_compute_logs/tvmps_53bb39c91e5339d39dd46dc46ed72174a017c78327fbfa2f7f38d3d35f9977ea_d/55_azureml-execution-tvmps_53bb39c91e5339d39dd46dc46ed72174a017c78327fbfa2f7f38d3d35f9977ea_d.txt\n",
      ">>>   2021/11/04 20:12:42 Set default ACL for files under directory by running: /usr/bin/setfacl -m default:g::rwx -m default:o::rwx /mnt/batch/tasks/shared/LS_root/jobs/hw2_pipeline/azureml/c7abcd3e-3aee-4283-8c53-f6c5f9204977/wd/azureml/c7abcd3e-3aee-4283-8c53-f6c5f9204977/azureml_compute_logs\n",
      ">>>   2021/11/04 20:12:42 Creating directory /mnt/batch/tasks/shared/LS_root/jobs/hw2_pipeline/azureml/c7abcd3e-3aee-4283-8c53-f6c5f9204977/wd/azureml/c7abcd3e-3aee-4283-8c53-f6c5f9204977/logs\n",
      ">>>   2021/11/04 20:12:42 Creating directory /mnt/batch/tasks/shared/LS_root/jobs/hw2_pipeline/azureml/c7abcd3e-3aee-4283-8c53-f6c5f9204977/wd/azureml/c7abcd3e-3aee-4283-8c53-f6c5f9204977/outputs\n",
      ">>>   2021/11/04 20:12:42 Starting output-watcher...\n",
      ">>>   2021/11/04 20:12:42 Single file input dataset is enabled.\n",
      ">>>   2021/11/04 20:12:42 SidecarEnabled:: isDetonationChamber: false, useDockerContainer: true\n",
      ">>>   2021/11/04 20:12:42 SidecarEnabled:: AmlDatasetContextManagerConfig exists: false\n",
      ">>>   2021/11/04 20:12:42 SidecarEnabled:: sidecar not enabled\n",
      ">>>   2021/11/04 20:12:42 Start to pulling docker image: 96758eaafa3647d4a83e17ccee49d173.azurecr.io/azureml/azureml_74b5d7b1bfec63ece12721fc9d263e9a\n",
      ">>>   2021/11/04 20:12:42 Start pull docker image: 96758eaafa3647d4a83e17ccee49d173.azurecr.io\n",
      ">>>   2021/11/04 20:12:42 Getting credentials for image 96758eaafa3647d4a83e17ccee49d173.azurecr.io/azureml/azureml_74b5d7b1bfec63ece12721fc9d263e9a with url 96758eaafa3647d4a83e17ccee49d173.azurecr.io\n",
      ">>>   2021/11/04 20:12:42 Container registry is ACR.\n",
      ">>>   2021/11/04 20:12:42 Skip getting ACR Credentials from Identity and will be getting it from EMS\n",
      ">>>   2021/11/04 20:12:42 Getting ACR Credentials from EMS for environment churn-pipeline-env:1\n",
      ">>>   2021/11/04 20:12:42 Requesting XDS for registry details.\n",
      ">>>   2021/11/04 20:12:42 Attempt 1 of http call to https://centralus.cert.api.azureml.ms/xdsbatchai/hosttoolapi/subscriptions/441cf665-f2bb-42da-9467-9dc54a3ae431/resourceGroups/pipeline/workspaces/hw2_pipeline/clusters/mlop-test/nodes/tvmps_53bb39c91e5339d39dd46dc46ed72174a017c78327fbfa2f7f38d3d35f9977ea_d?api-version=2018-02-01\n",
      ">>>   2021/11/04 20:12:42 Got container registry details from credentials service for registry address: 96758eaafa3647d4a83e17ccee49d173.azurecr.io.\n",
      ">>>   2021/11/04 20:12:42 Writing ACR Details to file...\n",
      ">>>   2021/11/04 20:12:42 Copying ACR Details file to worker nodes...\n",
      ">>>   2021/11/04 20:12:42 Executing 'Copy ACR Details file' on 10.0.0.4\n",
      ">>>   2021/11/04 20:12:42 Begin executing 'Copy ACR Details file' task on Node\n",
      ">>>   2021/11/04 20:12:42 'Copy ACR Details file' task Node result: succeeded\n",
      ">>>   2021/11/04 20:12:42 Copy ACR Details file succeeded on 10.0.0.4. Output: \n",
      ">>>   >>>   \n",
      ">>>   >>>   \n",
      ">>>   2021/11/04 20:12:42 Successfully retrieved ACR Credentials from EMS.\n",
      ">>>   2021/11/04 20:12:42 EMS returned 96758eaafa3647d4a83e17ccee49d173.azurecr.io for environment churn-pipeline-env\n",
      ">>>   2021/11/04 20:12:42 Save docker credentials for image 96758eaafa3647d4a83e17ccee49d173.azurecr.io/azureml/azureml_74b5d7b1bfec63ece12721fc9d263e9a in /mnt/batch/tasks/shared/LS_root/jobs/hw2_pipeline/azureml/c7abcd3e-3aee-4283-8c53-f6c5f9204977/wd/docker_login_794E0FFAE498A172\n",
      ">>>   2021/11/04 20:12:42 Start login to the docker registry\n",
      ">>>   2021/11/04 20:12:42 Successfully logged into the docker registry.\n",
      ">>>   2021/11/04 20:12:42 Start run pull docker image command\n",
      ">>>   2021/11/04 20:12:42 Pull docker image succeeded.\n",
      ">>>   2021/11/04 20:12:42 Removed docker config dir /mnt/batch/tasks/shared/LS_root/jobs/hw2_pipeline/azureml/c7abcd3e-3aee-4283-8c53-f6c5f9204977/wd/docker_login_794E0FFAE498A172\n",
      ">>>   2021/11/04 20:12:42 Pull docker image time: 280.63833ms\n",
      ">>>   \n",
      ">>>   2021/11/04 20:12:42 Docker Version that this nodes use are: 19.03.14+azure\n",
      ">>>   \n",
      ">>>   2021/11/04 20:12:42 The vmsize standard_ds11_v2 is not a GPU VM, skipping get GPU count by running nvidia-smi command.\n",
      ">>>   2021/11/04 20:12:42 The vmsize standard_ds11_v2 is not a GPU VM, skipping get GPU count by running nvidia-smi command.\n",
      ">>>   2021/11/04 20:12:42 Setting the memory limit for docker container to be 13674 MB\n",
      ">>>   2021/11/04 20:12:42 The env variable file size is 40943 bytes\n",
      ">>>   2021/11/04 20:12:42 Creating parent cgroup 'c7abcd3e-3aee-4283-8c53-f6c5f9204977' for Containers used in Job\n",
      ">>>   2021/11/04 20:12:42 Add parent cgroup 'c7abcd3e-3aee-4283-8c53-f6c5f9204977' to container 'c7abcd3e-3aee-4283-8c53-f6c5f9204977'\n",
      ">>>   2021/11/04 20:12:42 /dev/infiniband/uverbs0 found (implying presence of InfiniBand)?: false\n",
      ">>>   2021/11/04 20:12:42 Original Arguments: run,--ulimit,memlock=9223372036854775807,--ulimit,nofile=262144:262144,--cap-add,sys_ptrace,--name,c7abcd3e-3aee-4283-8c53-f6c5f9204977,-v,/mnt/batch/tasks/shared/LS_root/mounts:/mnt/batch/tasks/shared/LS_root/mounts:rshared,-v,/mnt/batch/tasks/shared/LS_root/configs:/mnt/batch/tasks/shared/LS_root/configs,-v,/mnt/batch/tasks/shared/LS_root/shared:/mnt/batch/tasks/shared/LS_root/shared,-v,/mnt/batch/tasks/workitems/67279c7f-ca32-4555-9f75-59da8a4c4229/job-1/c7abcd3e-3aee-4283-8_af76dac5-a51e-41b2-b62b-bba1cc4822d7/certs:/mnt/batch/tasks/workitems/67279c7f-ca32-4555-9f75-59da8a4c4229/job-1/c7abcd3e-3aee-4283-8_af76dac5-a51e-41b2-b62b-bba1cc4822d7/certs,-v,/mnt/batch/tasks/startup:/mnt/batch/tasks/startup,-m,13674m,-v,/mnt/batch/tasks/shared/LS_root/jobs/hw2_pipeline/azureml/c7abcd3e-3aee-4283-8c53-f6c5f9204977/wd/azureml/c7abcd3e-3aee-4283-8c53-f6c5f9204977/azureml_compute_logs:/mnt/batch/tasks/shared/LS_root/jobs/hw2_pipeline/azureml/c7abcd3e-3aee-4283-8c53-f6c5f9204977/wd/azureml/c7abcd3e-3aee-4283-8c53-f6c5f9204977/azureml_compute_logs,-v,/mnt/batch/tasks/workitems/67279c7f-ca32-4555-9f75-59da8a4c4229/job-1/c7abcd3e-3aee-4283-8_af76dac5-a51e-41b2-b62b-bba1cc4822d7/wd:/mnt/batch/tasks/workitems/67279c7f-ca32-4555-9f75-59da8a4c4229/job-1/c7abcd3e-3aee-4283-8_af76dac5-a51e-41b2-b62b-bba1cc4822d7/wd,-v,/mnt/batch/tasks/shared/LS_root/jobs/hw2_pipeline/azureml/c7abcd3e-3aee-4283-8c53-f6c5f9204977:/mnt/batch/tasks/shared/LS_root/jobs/hw2_pipeline/azureml/c7abcd3e-3aee-4283-8c53-f6c5f9204977,-v,/mnt/batch/tasks/shared/LS_root/shared/tracing/c7abcd3e-3aee-4283-8c53-f6c5f9204977/logs/azureml/tracing:/mnt/batch/tasks/shared/LS_root/shared/tracing/c7abcd3e-3aee-4283-8c53-f6c5f9204977/logs/azureml/tracing,-w,/mnt/batch/tasks/shared/LS_root/jobs/hw2_pipeline/azureml/c7abcd3e-3aee-4283-8c53-f6c5f9204977/wd,--expose,23,--env-file,/mnt/batch/tasks/shared/LS_root/jobs/hw2_pipeline/azureml/c7abcd3e-3aee-4283-8c53-f6c5f9204977/config/.batchai.envlist,--cgroup-parent=/c7abcd3e-3aee-4283-8c53-f6c5f9204977/,--shm-size,2g\n",
      ">>>   2021/11/04 20:12:42 the binding /mnt/batch/tasks/shared/LS_root/shared/tracing/c7abcd3e-3aee-4283-8c53-f6c5f9204977/logs/azureml/tracing:/mnt/batch/tasks/shared/LS_root/shared/tracing/c7abcd3e-3aee-4283-8c53-f6c5f9204977/logs/azureml/tracing is discarded as we already have /mnt/batch/tasks/shared/LS_root/shared:/mnt/batch/tasks/shared/LS_root/shared \n",
      ">>>   2021/11/04 20:12:42 the binding /mnt/batch/tasks/shared/LS_root/jobs/hw2_pipeline/azureml/c7abcd3e-3aee-4283-8c53-f6c5f9204977/wd/azureml/c7abcd3e-3aee-4283-8c53-f6c5f9204977/azureml_compute_logs:/mnt/batch/tasks/shared/LS_root/jobs/hw2_pipeline/azureml/c7abcd3e-3aee-4283-8c53-f6c5f9204977/wd/azureml/c7abcd3e-3aee-4283-8c53-f6c5f9204977/azureml_compute_logs is discarded as we already have /mnt/batch/tasks/shared/LS_root/jobs/hw2_pipeline/azureml/c7abcd3e-3aee-4283-8c53-f6c5f9204977:/mnt/batch/tasks/shared/LS_root/jobs/hw2_pipeline/azureml/c7abcd3e-3aee-4283-8c53-f6c5f9204977 \n",
      ">>>   2021/11/04 20:12:42 Updated Arguments: run,--ulimit,memlock=9223372036854775807,--ulimit,nofile=262144:262144,--cap-add,sys_ptrace,--name,c7abcd3e-3aee-4283-8c53-f6c5f9204977,-m,13674m,-w,/mnt/batch/tasks/shared/LS_root/jobs/hw2_pipeline/azureml/c7abcd3e-3aee-4283-8c53-f6c5f9204977/wd,--expose,23,--env-file,/mnt/batch/tasks/shared/LS_root/jobs/hw2_pipeline/azureml/c7abcd3e-3aee-4283-8c53-f6c5f9204977/config/.batchai.envlist,--cgroup-parent=/c7abcd3e-3aee-4283-8c53-f6c5f9204977/,--shm-size,2g,-v,/mnt/batch/tasks/startup:/mnt/batch/tasks/startup,-v,/mnt/batch/tasks/shared/LS_root/mounts:/mnt/batch/tasks/shared/LS_root/mounts:rshared,-v,/mnt/batch/tasks/shared/LS_root/shared:/mnt/batch/tasks/shared/LS_root/shared,-v,/mnt/batch/tasks/shared/LS_root/configs:/mnt/batch/tasks/shared/LS_root/configs,-v,/mnt/batch/tasks/shared/LS_root/jobs/hw2_pipeline/azureml/c7abcd3e-3aee-4283-8c53-f6c5f9204977:/mnt/batch/tasks/shared/LS_root/jobs/hw2_pipeline/azureml/c7abcd3e-3aee-4283-8c53-f6c5f9204977,-v,/mnt/batch/tasks/workitems/67279c7f-ca32-4555-9f75-59da8a4c4229/job-1/c7abcd3e-3aee-4283-8_af76dac5-a51e-41b2-b62b-bba1cc4822d7/wd:/mnt/batch/tasks/workitems/67279c7f-ca32-4555-9f75-59da8a4c4229/job-1/c7abcd3e-3aee-4283-8_af76dac5-a51e-41b2-b62b-bba1cc4822d7/wd,-v,/mnt/batch/tasks/workitems/67279c7f-ca32-4555-9f75-59da8a4c4229/job-1/c7abcd3e-3aee-4283-8_af76dac5-a51e-41b2-b62b-bba1cc4822d7/certs:/mnt/batch/tasks/workitems/67279c7f-ca32-4555-9f75-59da8a4c4229/job-1/c7abcd3e-3aee-4283-8_af76dac5-a51e-41b2-b62b-bba1cc4822d7/certs\n",
      ">>>   2021/11/04 20:12:42 Running Docker command: docker run --ulimit memlock=9223372036854775807 --ulimit nofile=262144:262144 --cap-add sys_ptrace --name c7abcd3e-3aee-4283-8c53-f6c5f9204977 -m 13674m -w /mnt/batch/tasks/shared/LS_root/jobs/hw2_pipeline/azureml/c7abcd3e-3aee-4283-8c53-f6c5f9204977/wd --expose 23 --env-file /mnt/batch/tasks/shared/LS_root/jobs/hw2_pipeline/azureml/c7abcd3e-3aee-4283-8c53-f6c5f9204977/config/.batchai.envlist --cgroup-parent=/c7abcd3e-3aee-4283-8c53-f6c5f9204977/ --shm-size 2g -v /mnt/batch/tasks/startup:/mnt/batch/tasks/startup -v /mnt/batch/tasks/shared/LS_root/mounts:/mnt/batch/tasks/shared/LS_root/mounts:rshared -v /mnt/batch/tasks/shared/LS_root/shared:/mnt/batch/tasks/shared/LS_root/shared -v /mnt/batch/tasks/shared/LS_root/configs:/mnt/batch/tasks/shared/LS_root/configs -v /mnt/batch/tasks/shared/LS_root/jobs/hw2_pipeline/azureml/c7abcd3e-3aee-4283-8c53-f6c5f9204977:/mnt/batch/tasks/shared/LS_root/jobs/hw2_pipeline/azureml/c7abcd3e-3aee-4283-8c53-f6c5f9204977 -v /mnt/batch/tasks/workitems/67279c7f-ca32-4555-9f75-59da8a4c4229/job-1/c7abcd3e-3aee-4283-8_af76dac5-a51e-41b2-b62b-bba1cc4822d7/wd:/mnt/batch/tasks/workitems/67279c7f-ca32-4555-9f75-59da8a4c4229/job-1/c7abcd3e-3aee-4283-8_af76dac5-a51e-41b2-b62b-bba1cc4822d7/wd -v /mnt/batch/tasks/workitems/67279c7f-ca32-4555-9f75-59da8a4c4229/job-1/c7abcd3e-3aee-4283-8_af76dac5-a51e-41b2-b62b-bba1cc4822d7/certs:/mnt/batch/tasks/workitems/67279c7f-ca32-4555-9f75-59da8a4c4229/job-1/c7abcd3e-3aee-4283-8_af76dac5-a51e-41b2-b62b-bba1cc4822d7/certs -d -it --privileged --net=host 96758eaafa3647d4a83e17ccee49d173.azurecr.io/azureml/azureml_74b5d7b1bfec63ece12721fc9d263e9a\n",
      ">>>   2021/11/04 20:12:42 Check if container c7abcd3e-3aee-4283-8c53-f6c5f9204977 already exist exited with 0, \n",
      ">>>   \n",
      ">>>   2021/11/04 20:12:42 Check if container c7abcd3e-3aee-4283-8c53-f6c5f9204977 already exist exited with 0, \n",
      ">>>   \n",
      ">>>   2021/11/04 20:12:42 Parameters for containerSetup task: useDetonationChamer set to false and sshRequired set to false \n",
      ">>>   2021/11/04 20:12:42 Parameters for containerSetup task: useDetonationChamer set to false and sshRequired set to false \n",
      ">>>   2021/11/04 20:12:42 containerSetup task cmd: [/mnt/batch/tasks/startup/wd/hosttools -task=containerSetup -traceContext=00-00eeb8c5c95823e4686eb3d63765f7a7-460f384b6b344b0f-01 -sshRequired=false] \n",
      ">>>   2021/11/04 20:12:42 containerSetup task cmd: [/mnt/batch/tasks/startup/wd/hosttools -task=containerSetup -traceContext=00-00eeb8c5c95823e4686eb3d63765f7a7-460f384b6b344b0f-01 -sshRequired=false] \n",
      ">>>   2021/11/04 20:12:43 Container ssh is not required for job type.\n",
      ">>>   2021/11/04 20:12:43 Starting docker container succeeded.\n",
      ">>>   2021/11/04 20:12:43 Starting docker container succeeded.\n",
      ">>>   2021/11/04 20:12:43 Disk space after starting docker container: 23316MB\n",
      ">>>   2021/11/04 20:12:43 SidecarEnabled:: isDetonationChamber: false, useDockerContainer: true\n",
      ">>>   2021/11/04 20:12:43 SidecarEnabled:: AmlDatasetContextManagerConfig exists: false\n",
      ">>>   2021/11/04 20:12:43 SidecarEnabled:: sidecar not enabled\n",
      ">>>   2021/11/04 20:12:43 Begin execution of runSpecialJobTask\n",
      ">>>   2021/11/04 20:12:43 Creating directory at $AZUREML_LOGDIRECTORY_PATH\n",
      ">>>   2021/11/04 20:12:43 Creating directory /mnt/batch/tasks/shared/LS_root/jobs/hw2_pipeline/azureml/c7abcd3e-3aee-4283-8c53-f6c5f9204977/wd/azureml/c7abcd3e-3aee-4283-8c53-f6c5f9204977/azureml-logs\n",
      ">>>   2021/11/04 20:12:43 runSpecialJobTask: os.GetEnv constants.StdouterrDir: /mnt/batch/tasks/shared/LS_root/jobs/hw2_pipeline/azureml/c7abcd3e-3aee-4283-8c53-f6c5f9204977/wd/azureml/c7abcd3e-3aee-4283-8c53-f6c5f9204977/azureml_compute_logs\n",
      ">>>   2021/11/04 20:12:43 runSpecialJobTask: Raw cmd for preparation is passed is: /azureml-envs/azureml_37123770cfa02a1fe00f423fa57ee472/bin/python /mnt/batch/tasks/shared/LS_root/jobs/hw2_pipeline/azureml/c7abcd3e-3aee-4283-8c53-f6c5f9204977/mounts/workspaceblobstore/azureml/c7abcd3e-3aee-4283-8c53-f6c5f9204977-setup/job_prep.py -i DataStoreCopy:context_managers.DataStores --snapshots '[{\"Id\":\"beb42c74-89f3-40d2-b4d7-f2b6ee525c2f\",\"PathStack\":[\".\"],\"SnapshotEntityId\":null}]'\n",
      ">>>   2021/11/04 20:12:43 runSpecialJobTask: stdout path for preparation is passed is: /mnt/batch/tasks/shared/LS_root/jobs/hw2_pipeline/azureml/c7abcd3e-3aee-4283-8c53-f6c5f9204977/wd/azureml/c7abcd3e-3aee-4283-8c53-f6c5f9204977/azureml_compute_logs/65_job_prep-tvmps_53bb39c91e5339d39dd46dc46ed72174a017c78327fbfa2f7f38d3d35f9977ea_d.txt\n",
      ">>>   2021/11/04 20:12:43 runSpecialJobTask: stderr path for preparation is passed is: /mnt/batch/tasks/shared/LS_root/jobs/hw2_pipeline/azureml/c7abcd3e-3aee-4283-8c53-f6c5f9204977/wd/azureml/c7abcd3e-3aee-4283-8c53-f6c5f9204977/azureml_compute_logs/65_job_prep-tvmps_53bb39c91e5339d39dd46dc46ed72174a017c78327fbfa2f7f38d3d35f9977ea_d.txt\n",
      ">>>   2021/11/04 20:12:43 native cmd: export AZUREML_JOB_TASK_ERROR_PATH='/mnt/batch/tasks/workitems/67279c7f-ca32-4555-9f75-59da8a4c4229/job-1/c7abcd3e-3aee-4283-8_af76dac5-a51e-41b2-b62b-bba1cc4822d7/wd/runSpecialJobTask_error.json';cd /mnt/batch/tasks/shared/LS_root/jobs/hw2_pipeline/azureml/c7abcd3e-3aee-4283-8c53-f6c5f9204977/wd/azureml/c7abcd3e-3aee-4283-8c53-f6c5f9204977;/azureml-envs/azureml_37123770cfa02a1fe00f423fa57ee472/bin/python /mnt/batch/tasks/shared/LS_root/jobs/hw2_pipeline/azureml/c7abcd3e-3aee-4283-8c53-f6c5f9204977/mounts/workspaceblobstore/azureml/c7abcd3e-3aee-4283-8c53-f6c5f9204977-setup/job_prep.py -i DataStoreCopy:context_managers.DataStores --snapshots '[{\"Id\":\"beb42c74-89f3-40d2-b4d7-f2b6ee525c2f\",\"PathStack\":[\".\"],\"SnapshotEntityId\":null}]'\n",
      ">>>   2021/11/04 20:12:43 runSpecialJobTask: commons.GetOsPlatform(): ubuntu\n",
      ">>>   2021/11/04 20:12:43 runSpecialJobTask: Running cmd: /usr/bin/docker exec -e AZUREML_SDK_TRACEPARENT=00-00eeb8c5c95823e4686eb3d63765f7a7-9560bb4feef8d806-01 -t c7abcd3e-3aee-4283-8c53-f6c5f9204977 bash -c if [ -f ~/.bashrc ]; then PS1_back=$PS1; PS1='$'; . ~/.bashrc; PS1=$PS1_back; fi;PATH=$PATH:$AZ_BATCH_NODE_STARTUP_DIR/wd/;export AZUREML_JOB_TASK_ERROR_PATH='/mnt/batch/tasks/workitems/67279c7f-ca32-4555-9f75-59da8a4c4229/job-1/c7abcd3e-3aee-4283-8_af76dac5-a51e-41b2-b62b-bba1cc4822d7/wd/runSpecialJobTask_error.json';cd /mnt/batch/tasks/shared/LS_root/jobs/hw2_pipeline/azureml/c7abcd3e-3aee-4283-8c53-f6c5f9204977/wd/azureml/c7abcd3e-3aee-4283-8c53-f6c5f9204977;/azureml-envs/azureml_37123770cfa02a1fe00f423fa57ee472/bin/python /mnt/batch/tasks/shared/LS_root/jobs/hw2_pipeline/azureml/c7abcd3e-3aee-4283-8c53-f6c5f9204977/mounts/workspaceblobstore/azureml/c7abcd3e-3aee-4283-8c53-f6c5f9204977-setup/job_prep.py -i DataStoreCopy:context_managers.DataStores --snapshots '[{\"Id\":\"beb42c74-89f3-40d2-b4d7-f2b6ee525c2f\",\"PathStack\":[\".\"],\"SnapshotEntityId\":null}]'\n",
      ">>>   2021/11/04 20:12:46 Attempt 1 of http call to https://centralus.api.azureml.ms/history/v1.0/private/subscriptions/441cf665-f2bb-42da-9467-9dc54a3ae431/resourceGroups/pipeline/providers/Microsoft.MachineLearningServices/workspaces/hw2_pipeline/runs/c7abcd3e-3aee-4283-8c53-f6c5f9204977/spans\n",
      ">>>   2021/11/04 20:12:48 containerName:c7abcd3e-3aee-4283-8c53-f6c5f9204977\n",
      ">>>   2021/11/04 20:12:48 sidecar containerName:c7abcd3e-3aee-4283-8c53-f6c5f9204977\n",
      ">>>   2021/11/04 20:12:48 Docker Version that this nodes use are: 19.03.14+azure\n",
      ">>>   \n",
      ">>>   2021/11/04 20:12:48 The vmsize standard_ds11_v2 is not a GPU VM, skipping get GPU count by running nvidia-smi command.\n",
      ">>>   2021/11/04 20:12:48 The vmsize standard_ds11_v2 is not a GPU VM, skipping get GPU count by running nvidia-smi command.\n",
      ">>>   2021/11/04 20:12:48 sidecar dockerLauncher:docker\n",
      ">>>   2021/11/04 20:12:48 sidecarContainerId:06d23bbc2b339736746557260e163ce7a8a928e60f1da7a708b62577116ffd98\n",
      ">>>   2021/11/04 20:12:48 Docker Version that this nodes use are: 19.03.14+azure\n",
      ">>>   \n",
      ">>>   2021/11/04 20:12:48 The vmsize standard_ds11_v2 is not a GPU VM, skipping get GPU count by running nvidia-smi command.\n",
      ">>>   2021/11/04 20:12:48 The vmsize standard_ds11_v2 is not a GPU VM, skipping get GPU count by running nvidia-smi command.\n",
      ">>>   2021/11/04 20:12:48 Docker logs for c7abcd3e-3aee-4283-8c53-f6c5f9204977\n",
      ">>>   \n",
      ">>>   2021/11/04 20:12:48 runSpecialJobTask: job preparation exited with code 0 and err <nil>\n",
      ">>>   \n",
      ">>>   2021/11/04 20:12:48 runSpecialJobTask: preparation: [2021-11-04T20:12:44.549709] Entering job preparation.\n",
      ">>>   2021/11/04 20:12:48 runSpecialJobTask: preparation: [2021-11-04T20:12:46.415933] Starting job preparation.\n",
      ">>>   2021/11/04 20:12:48 runSpecialJobTask: preparation: [2021-11-04T20:12:46.415968] Extracting the control code.\n",
      ">>>   2021/11/04 20:12:48 runSpecialJobTask: preparation: [2021-11-04T20:12:46.416246] Starting extract_project.\n",
      ">>>   2021/11/04 20:12:48 runSpecialJobTask: preparation: [2021-11-04T20:12:46.416287] Starting to extract zip file.\n",
      ">>>   2021/11/04 20:12:48 runSpecialJobTask: preparation: [2021-11-04T20:12:46.435342] Finished extracting zip file.\n",
      ">>>   2021/11/04 20:12:48 runSpecialJobTask: preparation: [2021-11-04T20:12:46.438405] Using urllib.request Python 3.0 or later\n",
      ">>>   2021/11/04 20:12:48 runSpecialJobTask: preparation: [2021-11-04T20:12:46.438444] Start fetching snapshots.\n",
      ">>>   2021/11/04 20:12:48 runSpecialJobTask: preparation: [2021-11-04T20:12:46.438486] Start fetching snapshot.\n",
      ">>>   2021/11/04 20:12:48 runSpecialJobTask: preparation: [2021-11-04T20:12:46.438522] Retrieving project from snapshot: beb42c74-89f3-40d2-b4d7-f2b6ee525c2f\n",
      ">>>   2021/11/04 20:12:48 runSpecialJobTask: preparation: Starting the daemon thread to refresh tokens in background for process with pid = 44\n",
      ">>>   2021/11/04 20:12:48 runSpecialJobTask: preparation: [2021-11-04T20:12:47.844417] Finished fetching snapshot.\n",
      ">>>   2021/11/04 20:12:48 runSpecialJobTask: preparation: [2021-11-04T20:12:47.844453] Finished fetching snapshots.\n",
      ">>>   2021/11/04 20:12:48 runSpecialJobTask: preparation: [2021-11-04T20:12:47.844460] Finished extract_project.\n",
      ">>>   2021/11/04 20:12:48 runSpecialJobTask: preparation: [2021-11-04T20:12:47.844633] Finished fetching and extracting the control code.\n",
      ">>>   2021/11/04 20:12:48 runSpecialJobTask: preparation: [2021-11-04T20:12:47.847975] downloadDataStore - Download from datastores if requested.\n",
      ">>>   2021/11/04 20:12:48 runSpecialJobTask: preparation: [2021-11-04T20:12:47.849160] Start run_history_prep.\n",
      ">>>   2021/11/04 20:12:48 runSpecialJobTask: preparation: [2021-11-04T20:12:47.860729] Entering context manager injector.\n",
      ">>>   2021/11/04 20:12:48 runSpecialJobTask: preparation: Acquired lockfile /tmp/c7abcd3e-3aee-4283-8c53-f6c5f9204977-datastore.lock to downloading input data references\n",
      ">>>   2021/11/04 20:12:48 runSpecialJobTask: preparation: [2021-11-04T20:12:48.287083] downloadDataStore completed\n",
      ">>>   2021/11/04 20:12:48 runSpecialJobTask: preparation: [2021-11-04T20:12:48.290076] Job preparation is complete.\n",
      ">>>   2021/11/04 20:12:48 DockerSideCarContainerLogs:\n",
      ">>>   \n",
      ">>>   2021/11/04 20:12:48 DockerSideCarContainerLogs End\n",
      ">>>   2021/11/04 20:12:48 Execution of runSpecialJobTask completed\n",
      ">>>   2021/11/04 20:12:48 Not exporting to RunHistory as the exporter is either stopped or there is no data.\n",
      ">>>   Stopped: false\n",
      ">>>   OriginalData: 3\n",
      ">>>   FilteredData: 0.\n",
      ">>>   2021/11/04 20:12:48 Process Exiting with Code:  0\n",
      ">>>   2021/11/04 20:12:49 All App Insights Logs was sent successfully or the close timeout of 10 was reached\n",
      ">>>   \n",
      "2021-11-04T20:12:49Z The vmsize standard_ds11_v2 is not a GPU VM, skipping get GPU count by running nvidia-smi command.\n",
      "2021-11-04T20:12:49Z The vmsize standard_ds11_v2 is not a GPU VM, skipping get GPU count by running nvidia-smi command.\n",
      "2021-11-04T20:12:49Z 127.0.0.1 slots=2 max-slots=2\n",
      "2021-11-04T20:12:49Z launching Custom job\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Streaming azureml-logs/75_job_post-tvmps_53bb39c91e5339d39dd46dc46ed72174a017c78327fbfa2f7f38d3d35f9977ea_d.txt\n",
      "===============================================================================================================\n",
      "[2021-11-04T20:13:05.647853] Entering job release\n",
      "[2021-11-04T20:13:06.436964] Starting job release\n",
      "[2021-11-04T20:13:06.437579] Logging experiment finalizing status in history service.\n",
      "Starting the daemon thread to refresh tokens in background for process with pid = 318\n",
      "[2021-11-04T20:13:06.437955] job release stage : upload_datastore starting...\n",
      "[2021-11-04T20:13:06.445438] Entering context manager injector.\n",
      "[2021-11-04T20:13:06.446924] job release stage : start importing azureml.history._tracking in run_history_release.\n",
      "[2021-11-04T20:13:06.448131] job release stage : execute_job_release starting...\n",
      "[2021-11-04T20:13:06.448272] job release stage : copy_batchai_cached_logs starting...\n",
      "[2021-11-04T20:13:06.448910] job release stage : copy_batchai_cached_logs completed...\n",
      "[2021-11-04T20:13:06.469445] job release stage : upload_datastore completed...\n",
      "[2021-11-04T20:13:06.529998] job release stage : send_run_telemetry starting...\n",
      "[2021-11-04T20:13:06.551270] get vm size and vm region successfully.\n",
      "[2021-11-04T20:13:06.558747] get compute meta data successfully.\n",
      "[2021-11-04T20:13:06.649681] job release stage : execute_job_release completed...\n",
      "[2021-11-04T20:13:06.751399] post artifact meta request successfully.\n",
      "[2021-11-04T20:13:06.841947] upload compute record artifact successfully.\n",
      "[2021-11-04T20:13:06.842006] job release stage : send_run_telemetry completed...\n",
      "[2021-11-04T20:13:06.842172] Job release is complete\n",
      "\n",
      "StepRun(Prepare Data) Execution Summary\n",
      "========================================\n",
      "StepRun( Prepare Data ) Status: Finished\n",
      "{'runId': 'c7abcd3e-3aee-4283-8c53-f6c5f9204977', 'target': 'mlop-test', 'status': 'Completed', 'startTimeUtc': '2021-11-04T20:12:40.367494Z', 'endTimeUtc': '2021-11-04T20:13:17.265115Z', 'services': {}, 'properties': {'ContentSnapshotId': 'beb42c74-89f3-40d2-b4d7-f2b6ee525c2f', 'StepType': 'PythonScriptStep', 'ComputeTargetType': 'AmlCompute', 'azureml.moduleid': 'd8c28e41-2897-461a-994b-0610a70b869a', 'azureml.runsource': 'azureml.StepRun', 'azureml.nodeid': 'd23f3212', 'azureml.pipelinerunid': '2f4f914e-7841-416d-8404-90e36c983bc4', 'azureml.pipeline': '2f4f914e-7841-416d-8404-90e36c983bc4', 'azureml.pipelineComponent': 'masterescloud', '_azureml.ComputeTargetType': 'amlcompute', 'ProcessInfoFile': 'azureml-logs/process_info.json', 'ProcessStatusFile': 'azureml-logs/process_status.json'}, 'inputDatasets': [{'dataset': {'id': '904b6f9c-bdb3-46a6-9f89-e68d52fa3f0c'}, 'consumptionDetails': {'type': 'RunInput', 'inputName': 'raw_data', 'mechanism': 'Direct'}}], 'outputDatasets': [], 'runDefinition': {'script': 'prep_churn.py', 'command': '', 'useAbsolutePath': False, 'arguments': ['--input-data', 'DatasetConsumptionConfig:raw_data', '--prepped-data', '$AZUREML_DATAREFERENCE_prepped_data_folder'], 'sourceDirectoryDataStore': None, 'framework': 'Python', 'communicator': 'None', 'target': 'mlop-test', 'dataReferences': {'prepped_data_folder': {'dataStoreName': 'workspaceblobstore', 'mode': 'Mount', 'pathOnDataStore': 'azureml/c7abcd3e-3aee-4283-8c53-f6c5f9204977/prepped_data_folder', 'pathOnCompute': None, 'overwrite': False}}, 'data': {'raw_data': {'dataLocation': {'dataset': {'id': '904b6f9c-bdb3-46a6-9f89-e68d52fa3f0c', 'name': None, 'version': '1'}, 'dataPath': None, 'uri': None}, 'mechanism': 'Direct', 'environmentVariableName': 'raw_data', 'pathOnCompute': None, 'overwrite': False, 'options': None}}, 'outputData': {}, 'datacaches': [], 'jobName': None, 'maxRunDurationSeconds': None, 'nodeCount': 1, 'instanceTypes': [], 'priority': None, 'credentialPassthrough': False, 'identity': None, 'environment': {'name': 'churn-pipeline-env', 'version': '1', 'python': {'interpreterPath': 'python', 'userManagedDependencies': False, 'condaDependencies': {'channels': ['anaconda', 'conda-forge'], 'dependencies': ['python=3.6.2', {'pip': ['azureml-defaults', 'azureml-dataprep[pandas]', 'pyarrow']}, 'scikit-learn', 'ipykernel', 'matplotlib', 'pandas', 'pip'], 'name': 'azureml_37123770cfa02a1fe00f423fa57ee472'}, 'baseCondaEnvironment': None}, 'environmentVariables': {'EXAMPLE_ENV_VAR': 'EXAMPLE_VALUE'}, 'docker': {'baseImage': 'mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04:20210922.v1', 'platform': {'os': 'Linux', 'architecture': 'amd64'}, 'baseDockerfile': None, 'baseImageRegistry': {'address': None, 'username': None, 'password': None}, 'enabled': False, 'arguments': []}, 'spark': {'repositories': [], 'packages': [], 'precachePackages': True}}, 'history': {'outputCollection': True, 'directoriesToWatch': ['logs'], 'enableMLflowTracking': True, 'snapshotProject': True}, 'spark': {'configuration': {'spark.app.name': 'Azure ML Experiment', 'spark.yarn.maxAppAttempts': '1'}}, 'parallelTask': {'maxRetriesPerWorker': 0, 'workerCountPerNode': 1, 'terminalExitCodes': None, 'configuration': {}}, 'amlCompute': {'name': None, 'vmSize': None, 'retainCluster': False, 'clusterMaxNodeCount': 1}, 'aiSuperComputer': {'instanceType': 'D2', 'imageVersion': 'pytorch-1.7.0', 'location': None, 'aiSuperComputerStorageData': None, 'interactive': False, 'scalePolicy': None, 'virtualClusterArmId': None, 'tensorboardLogDirectory': None, 'sshPublicKey': None, 'sshPublicKeys': None, 'enableAzmlInt': True, 'priority': 'Medium', 'slaTier': 'Standard', 'userAlias': None}, 'kubernetesCompute': {'instanceType': None}, 'tensorflow': {'workerCount': 1, 'parameterServerCount': 1}, 'mpi': {'processCountPerNode': 1}, 'pyTorch': {'communicationBackend': 'nccl', 'processCount': None}, 'hdi': {'yarnDeployMode': 'Cluster'}, 'containerInstance': {'region': None, 'cpuCores': 2.0, 'memoryGb': 3.5}, 'exposedPorts': None, 'docker': {'useDocker': False, 'sharedVolumes': True, 'shmSize': '2g', 'arguments': []}, 'cmk8sCompute': {'configuration': {}}, 'commandReturnCodeConfig': {'returnCode': 'Zero', 'successfulReturnCodes': []}, 'environmentVariables': {}, 'applicationEndpoints': {}, 'parameters': []}, 'logFiles': {'azureml-logs/55_azureml-execution-tvmps_53bb39c91e5339d39dd46dc46ed72174a017c78327fbfa2f7f38d3d35f9977ea_d.txt': 'https://hw2pipeline7446083280.blob.core.windows.net/azureml/ExperimentRun/dcid.c7abcd3e-3aee-4283-8c53-f6c5f9204977/azureml-logs/55_azureml-execution-tvmps_53bb39c91e5339d39dd46dc46ed72174a017c78327fbfa2f7f38d3d35f9977ea_d.txt?sv=2019-07-07&sr=b&sig=IxkXFqZCvxBJMzYiY1ASW4ZF80EixjniKMK0Tfrq0wQ%3D&skoid=f9378829-dfb7-42de-8f6c-6d4577bab050&sktid=6d7adc86-8446-4cb9-9972-1477c831ff28&skt=2021-11-04T14%3A14%3A36Z&ske=2021-11-05T22%3A24%3A36Z&sks=b&skv=2019-07-07&st=2021-11-04T20%3A03%3A09Z&se=2021-11-05T04%3A13%3A09Z&sp=r', 'azureml-logs/65_job_prep-tvmps_53bb39c91e5339d39dd46dc46ed72174a017c78327fbfa2f7f38d3d35f9977ea_d.txt': 'https://hw2pipeline7446083280.blob.core.windows.net/azureml/ExperimentRun/dcid.c7abcd3e-3aee-4283-8c53-f6c5f9204977/azureml-logs/65_job_prep-tvmps_53bb39c91e5339d39dd46dc46ed72174a017c78327fbfa2f7f38d3d35f9977ea_d.txt?sv=2019-07-07&sr=b&sig=7SnBtZx9DH94vEhv3zDw5rMxgbldXWxFB9HLshC5DTc%3D&skoid=f9378829-dfb7-42de-8f6c-6d4577bab050&sktid=6d7adc86-8446-4cb9-9972-1477c831ff28&skt=2021-11-04T14%3A14%3A36Z&ske=2021-11-05T22%3A24%3A36Z&sks=b&skv=2019-07-07&st=2021-11-04T20%3A03%3A09Z&se=2021-11-05T04%3A13%3A09Z&sp=r', 'azureml-logs/70_driver_log.txt': 'https://hw2pipeline7446083280.blob.core.windows.net/azureml/ExperimentRun/dcid.c7abcd3e-3aee-4283-8c53-f6c5f9204977/azureml-logs/70_driver_log.txt?sv=2019-07-07&sr=b&sig=U11%2Fr5H89fNktcUgscGbHVcVbXGU7OX3HBEPj2UW%2BJc%3D&skoid=f9378829-dfb7-42de-8f6c-6d4577bab050&sktid=6d7adc86-8446-4cb9-9972-1477c831ff28&skt=2021-11-04T14%3A14%3A36Z&ske=2021-11-05T22%3A24%3A36Z&sks=b&skv=2019-07-07&st=2021-11-04T20%3A03%3A09Z&se=2021-11-05T04%3A13%3A09Z&sp=r', 'azureml-logs/75_job_post-tvmps_53bb39c91e5339d39dd46dc46ed72174a017c78327fbfa2f7f38d3d35f9977ea_d.txt': 'https://hw2pipeline7446083280.blob.core.windows.net/azureml/ExperimentRun/dcid.c7abcd3e-3aee-4283-8c53-f6c5f9204977/azureml-logs/75_job_post-tvmps_53bb39c91e5339d39dd46dc46ed72174a017c78327fbfa2f7f38d3d35f9977ea_d.txt?sv=2019-07-07&sr=b&sig=Dc8dGEED%2Bj5OtgJeabSaxR4okBceIX7F%2Bh4Yn5DyVRw%3D&skoid=f9378829-dfb7-42de-8f6c-6d4577bab050&sktid=6d7adc86-8446-4cb9-9972-1477c831ff28&skt=2021-11-04T14%3A14%3A36Z&ske=2021-11-05T22%3A24%3A36Z&sks=b&skv=2019-07-07&st=2021-11-04T20%3A03%3A09Z&se=2021-11-05T04%3A13%3A09Z&sp=r', 'azureml-logs/process_info.json': 'https://hw2pipeline7446083280.blob.core.windows.net/azureml/ExperimentRun/dcid.c7abcd3e-3aee-4283-8c53-f6c5f9204977/azureml-logs/process_info.json?sv=2019-07-07&sr=b&sig=b%2Be3vQOIXQm1x12E%2FHZHJHVTQKnr6ujpqqM2tKH7Zms%3D&skoid=f9378829-dfb7-42de-8f6c-6d4577bab050&sktid=6d7adc86-8446-4cb9-9972-1477c831ff28&skt=2021-11-04T14%3A14%3A36Z&ske=2021-11-05T22%3A24%3A36Z&sks=b&skv=2019-07-07&st=2021-11-04T20%3A03%3A09Z&se=2021-11-05T04%3A13%3A09Z&sp=r', 'azureml-logs/process_status.json': 'https://hw2pipeline7446083280.blob.core.windows.net/azureml/ExperimentRun/dcid.c7abcd3e-3aee-4283-8c53-f6c5f9204977/azureml-logs/process_status.json?sv=2019-07-07&sr=b&sig=hppnBHfgKcFoGNLl4frCmsfQ%2B0%2B0rxUnR1T6ft72u2A%3D&skoid=f9378829-dfb7-42de-8f6c-6d4577bab050&sktid=6d7adc86-8446-4cb9-9972-1477c831ff28&skt=2021-11-04T14%3A14%3A36Z&ske=2021-11-05T22%3A24%3A36Z&sks=b&skv=2019-07-07&st=2021-11-04T20%3A03%3A09Z&se=2021-11-05T04%3A13%3A09Z&sp=r', 'logs/azureml/98_azureml.log': 'https://hw2pipeline7446083280.blob.core.windows.net/azureml/ExperimentRun/dcid.c7abcd3e-3aee-4283-8c53-f6c5f9204977/logs/azureml/98_azureml.log?sv=2019-07-07&sr=b&sig=3LvpYK%2Bx%2F5IvIuZCTLTh9BoiGOquBKPuOIah2pFrbSY%3D&skoid=f9378829-dfb7-42de-8f6c-6d4577bab050&sktid=6d7adc86-8446-4cb9-9972-1477c831ff28&skt=2021-11-04T14%3A20%3A43Z&ske=2021-11-05T22%3A30%3A43Z&sks=b&skv=2019-07-07&st=2021-11-04T20%3A03%3A09Z&se=2021-11-05T04%3A13%3A09Z&sp=r', 'logs/azureml/dataprep/backgroundProcess.log': 'https://hw2pipeline7446083280.blob.core.windows.net/azureml/ExperimentRun/dcid.c7abcd3e-3aee-4283-8c53-f6c5f9204977/logs/azureml/dataprep/backgroundProcess.log?sv=2019-07-07&sr=b&sig=Xz3kaiqqxJtSA%2FLoWuFxoQBZ90Ub40S6Q0FlsBzyNaw%3D&skoid=f9378829-dfb7-42de-8f6c-6d4577bab050&sktid=6d7adc86-8446-4cb9-9972-1477c831ff28&skt=2021-11-04T14%3A20%3A43Z&ske=2021-11-05T22%3A30%3A43Z&sks=b&skv=2019-07-07&st=2021-11-04T20%3A03%3A09Z&se=2021-11-05T04%3A13%3A09Z&sp=r', 'logs/azureml/dataprep/backgroundProcess_Telemetry.log': 'https://hw2pipeline7446083280.blob.core.windows.net/azureml/ExperimentRun/dcid.c7abcd3e-3aee-4283-8c53-f6c5f9204977/logs/azureml/dataprep/backgroundProcess_Telemetry.log?sv=2019-07-07&sr=b&sig=8Ws4fk95FCVRf%2B8je%2FLaxUd58jxNvQTih47Q6GoZixc%3D&skoid=f9378829-dfb7-42de-8f6c-6d4577bab050&sktid=6d7adc86-8446-4cb9-9972-1477c831ff28&skt=2021-11-04T14%3A20%3A43Z&ske=2021-11-05T22%3A30%3A43Z&sks=b&skv=2019-07-07&st=2021-11-04T20%3A03%3A09Z&se=2021-11-05T04%3A13%3A09Z&sp=r', 'logs/azureml/executionlogs.txt': 'https://hw2pipeline7446083280.blob.core.windows.net/azureml/ExperimentRun/dcid.c7abcd3e-3aee-4283-8c53-f6c5f9204977/logs/azureml/executionlogs.txt?sv=2019-07-07&sr=b&sig=vybzS4IAIioM24ZrnAPFfB25R7WtEqMnsjeK1BeSpGw%3D&skoid=f9378829-dfb7-42de-8f6c-6d4577bab050&sktid=6d7adc86-8446-4cb9-9972-1477c831ff28&skt=2021-11-04T14%3A20%3A43Z&ske=2021-11-05T22%3A30%3A43Z&sks=b&skv=2019-07-07&st=2021-11-04T20%3A03%3A09Z&se=2021-11-05T04%3A13%3A09Z&sp=r', 'logs/azureml/job_prep_azureml.log': 'https://hw2pipeline7446083280.blob.core.windows.net/azureml/ExperimentRun/dcid.c7abcd3e-3aee-4283-8c53-f6c5f9204977/logs/azureml/job_prep_azureml.log?sv=2019-07-07&sr=b&sig=p%2BE0u3iIQEkN%2B4vgff%2FTdilB4IFlSyaLWqvdd5tmdRo%3D&skoid=f9378829-dfb7-42de-8f6c-6d4577bab050&sktid=6d7adc86-8446-4cb9-9972-1477c831ff28&skt=2021-11-04T14%3A20%3A43Z&ske=2021-11-05T22%3A30%3A43Z&sks=b&skv=2019-07-07&st=2021-11-04T20%3A03%3A09Z&se=2021-11-05T04%3A13%3A09Z&sp=r', 'logs/azureml/job_release_azureml.log': 'https://hw2pipeline7446083280.blob.core.windows.net/azureml/ExperimentRun/dcid.c7abcd3e-3aee-4283-8c53-f6c5f9204977/logs/azureml/job_release_azureml.log?sv=2019-07-07&sr=b&sig=nYMmg5dr8rVJp1uIrvXv%2FYPGbXaKro23isX8Fjyiq3E%3D&skoid=f9378829-dfb7-42de-8f6c-6d4577bab050&sktid=6d7adc86-8446-4cb9-9972-1477c831ff28&skt=2021-11-04T14%3A20%3A43Z&ske=2021-11-05T22%3A30%3A43Z&sks=b&skv=2019-07-07&st=2021-11-04T20%3A03%3A09Z&se=2021-11-05T04%3A13%3A09Z&sp=r', 'logs/azureml/stderrlogs.txt': 'https://hw2pipeline7446083280.blob.core.windows.net/azureml/ExperimentRun/dcid.c7abcd3e-3aee-4283-8c53-f6c5f9204977/logs/azureml/stderrlogs.txt?sv=2019-07-07&sr=b&sig=gzVrY2DGYd3AJE8jA7CEQDxq5gWwba9Ha7N%2B8O1iAfk%3D&skoid=f9378829-dfb7-42de-8f6c-6d4577bab050&sktid=6d7adc86-8446-4cb9-9972-1477c831ff28&skt=2021-11-04T14%3A20%3A43Z&ske=2021-11-05T22%3A30%3A43Z&sks=b&skv=2019-07-07&st=2021-11-04T20%3A03%3A09Z&se=2021-11-05T04%3A13%3A09Z&sp=r', 'logs/azureml/stdoutlogs.txt': 'https://hw2pipeline7446083280.blob.core.windows.net/azureml/ExperimentRun/dcid.c7abcd3e-3aee-4283-8c53-f6c5f9204977/logs/azureml/stdoutlogs.txt?sv=2019-07-07&sr=b&sig=OkG3BiqozCICmo%2ByH6UhGJpMqY0JJ7dyHUaRq7R07EM%3D&skoid=f9378829-dfb7-42de-8f6c-6d4577bab050&sktid=6d7adc86-8446-4cb9-9972-1477c831ff28&skt=2021-11-04T14%3A20%3A43Z&ske=2021-11-05T22%3A30%3A43Z&sks=b&skv=2019-07-07&st=2021-11-04T20%3A03%3A09Z&se=2021-11-05T04%3A13%3A09Z&sp=r'}, 'submittedBy': 'Junrui Wan'}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "StepRunId: 833eb808-914e-4d07-b1ec-bc27dd7bcfa9\n",
      "Link to Azure Machine Learning Portal: https://ml.azure.com/runs/833eb808-914e-4d07-b1ec-bc27dd7bcfa9?wsid=/subscriptions/441cf665-f2bb-42da-9467-9dc54a3ae431/resourcegroups/pipeline/workspaces/hw2_pipeline&tid=6d7adc86-8446-4cb9-9972-1477c831ff28\n",
      "StepRun( Train and Register Model ) Status: Running\n",
      "\n",
      "Streaming azureml-logs/55_azureml-execution-tvmps_53bb39c91e5339d39dd46dc46ed72174a017c78327fbfa2f7f38d3d35f9977ea_d.txt\n",
      "========================================================================================================================\n",
      "2021-11-04T20:14:51Z Running following command: /bin/bash -c sudo blobfuse /mnt/batch/tasks/shared/LS_root/jobs/hw2_pipeline/azureml/833eb808-914e-4d07-b1ec-bc27dd7bcfa9/mounts/workspaceblobstore --tmp-path=/mnt/batch/tasks/shared/LS_root/jobs/hw2_pipeline/azureml/833eb808-914e-4d07-b1ec-bc27dd7bcfa9/caches/workspaceblobstore --file-cache-timeout-in-seconds=1000000 --cache-size-mb=21862 -o nonempty -o allow_other --config-file=/mnt/batch/tasks/shared/LS_root/jobs/hw2_pipeline/azureml/833eb808-914e-4d07-b1ec-bc27dd7bcfa9/configs/workspaceblobstore.cfg --log-level=LOG_WARNING\n",
      "2021-11-04T20:14:51Z Successfully mounted a/an Blobfuse File System at /mnt/batch/tasks/shared/LS_root/jobs/hw2_pipeline/azureml/833eb808-914e-4d07-b1ec-bc27dd7bcfa9/mounts/workspaceblobstore\n",
      "2021-11-04T20:14:52Z The vmsize standard_ds11_v2 is not a GPU VM, skipping get GPU count by running nvidia-smi command.\n",
      "2021-11-04T20:14:52Z Starting output-watcher...\n",
      "2021-11-04T20:14:52Z IsDedicatedCompute == True, won't poll for Low Pri Preemption\n",
      "2021-11-04T20:14:52Z Executing 'Copy ACR Details file' on 10.0.0.4\n",
      "2021-11-04T20:14:52Z Copy ACR Details file succeeded on 10.0.0.4. Output: \n",
      ">>>   \n",
      ">>>   \n",
      "\n",
      "Streaming azureml-logs/75_job_post-tvmps_53bb39c91e5339d39dd46dc46ed72174a017c78327fbfa2f7f38d3d35f9977ea_d.txt\n",
      "===============================================================================================================\n",
      "[2021-11-04T20:15:05.081131] Entering job release\n",
      "[2021-11-04T20:15:05.874448] Starting job release\n",
      "[2021-11-04T20:15:05.875028] Logging experiment finalizing status in history service.\n",
      "Starting the daemon thread to refresh tokens in background for process with pid = 135\n",
      "[2021-11-04T20:15:05.875481] job release stage : upload_datastore starting...\n",
      "[2021-11-04T20:15:05.882555] job release stage : start importing azureml.history._tracking in run_history_release.\n",
      "[2021-11-04T20:15:05.883708] job release stage : execute_job_release starting...\n",
      "[2021-11-04T20:15:05.884092] Entering context manager injector.\n",
      "[2021-11-04T20:15:05.886126] job release stage : copy_batchai_cached_logs starting...\n",
      "[2021-11-04T20:15:05.886455] job release stage : copy_batchai_cached_logs completed...\n",
      "[2021-11-04T20:15:05.904802] job release stage : upload_datastore completed...\n",
      "[2021-11-04T20:15:05.943243] job release stage : send_run_telemetry starting...\n",
      "[2021-11-04T20:15:05.963512] get vm size and vm region successfully.\n",
      "[2021-11-04T20:15:05.969846] get compute meta data successfully.\n",
      "[2021-11-04T20:15:06.088201] job release stage : execute_job_release completed...\n",
      "[2021-11-04T20:15:06.230065] post artifact meta request successfully.\n",
      "[2021-11-04T20:15:06.267424] upload compute record artifact successfully.\n",
      "[2021-11-04T20:15:06.267477] job release stage : send_run_telemetry completed...\n",
      "[2021-11-04T20:15:06.267735] Job release is complete\n",
      "\n",
      "StepRun(Train and Register Model) Execution Summary\n",
      "====================================================\n",
      "StepRun( Train and Register Model ) Status: Failed\n",
      "\n",
      "Warnings:\n",
      "{\n",
      "  \"error\": {\n",
      "    \"code\": \"UserError\",\n",
      "    \"severity\": null,\n",
      "    \"message\": \"AzureMLCompute job failed.\\nJobFailed: Submitted script failed with a non-zero exit code; see the driver log file for details.\\n\\tReason: Job failed with non-zero exit Code\",\n",
      "    \"messageFormat\": \"{Message}\",\n",
      "    \"messageParameters\": {\n",
      "      \"Message\": \"AzureMLCompute job failed.\\nJobFailed: Submitted script failed with a non-zero exit code; see the driver log file for details.\\n\\tReason: Job failed with non-zero exit Code\"\n",
      "    },\n",
      "    \"referenceCode\": null,\n",
      "    \"detailsUri\": null,\n",
      "    \"target\": null,\n",
      "    \"details\": [],\n",
      "    \"innerError\": {\n",
      "      \"code\": \"UserTrainingScriptFailed\",\n",
      "      \"innerError\": null\n",
      "    },\n",
      "    \"debugInfo\": null,\n",
      "    \"additionalInfo\": null\n",
      "  },\n",
      "  \"correlation\": {\n",
      "    \"operation\": \"fccef436ed603f49a477c5dfa27c7913\",\n",
      "    \"request\": \"040808a451a485e9\"\n",
      "  },\n",
      "  \"environment\": \"centralus\",\n",
      "  \"location\": \"centralus\",\n",
      "  \"time\": \"2021-11-04T20:15:15.9836171+00:00\",\n",
      "  \"componentName\": \"globaljobdispatcher\"\n",
      "}\n"
     ]
    },
    {
     "ename": "ActivityFailedException",
     "evalue": "ActivityFailedException:\n\tMessage: Activity Failed:\n{\n    \"error\": {\n        \"code\": \"UserError\",\n        \"message\": \"User program failed with KeyError: 'Attrition_Yes'\",\n        \"messageParameters\": {},\n        \"detailsUri\": \"https://aka.ms/azureml-run-troubleshooting\",\n        \"details\": []\n    },\n    \"time\": \"0001-01-01T00:00:00.000Z\"\n}\n\tInnerException None\n\tErrorResponse \n{\n    \"error\": {\n        \"message\": \"Activity Failed:\\n{\\n    \\\"error\\\": {\\n        \\\"code\\\": \\\"UserError\\\",\\n        \\\"message\\\": \\\"User program failed with KeyError: 'Attrition_Yes'\\\",\\n        \\\"messageParameters\\\": {},\\n        \\\"detailsUri\\\": \\\"https://aka.ms/azureml-run-troubleshooting\\\",\\n        \\\"details\\\": []\\n    },\\n    \\\"time\\\": \\\"0001-01-01T00:00:00.000Z\\\"\\n}\"\n    }\n}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mActivityFailedException\u001b[0m                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-8c51392c5451>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Pipeline submitted for execution.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mRunDetails\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipeline_run\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mpipeline_run\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_for_completion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshow_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/azureml/pipeline/core/run.py\u001b[0m in \u001b[0;36mwait_for_completion\u001b[0;34m(self, show_output, timeout_seconds, raise_on_error)\u001b[0m\n\u001b[1;32m    293\u001b[0m                             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m                             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m                                 step_run.wait_for_completion(timeout_seconds=timeout_seconds - time_elapsed,\n\u001b[0m\u001b[1;32m    296\u001b[0m                                                              raise_on_error=raise_on_error)\n\u001b[1;32m    297\u001b[0m                             \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/azureml/pipeline/core/run.py\u001b[0m in \u001b[0;36mwait_for_completion\u001b[0;34m(self, show_output, timeout_seconds, raise_on_error)\u001b[0m\n\u001b[1;32m    735\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mshow_output\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 737\u001b[0;31m                 return self._stream_run_output(timeout_seconds=timeout_seconds,\n\u001b[0m\u001b[1;32m    738\u001b[0m                                                raise_on_error=raise_on_error)\n\u001b[1;32m    739\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/azureml/pipeline/core/run.py\u001b[0m in \u001b[0;36m_stream_run_output\u001b[0;34m(self, timeout_seconds, raise_on_error)\u001b[0m\n\u001b[1;32m    828\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    829\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0merror\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mraise_on_error\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 830\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mActivityFailedException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_details\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    831\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_details\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mActivityFailedException\u001b[0m: ActivityFailedException:\n\tMessage: Activity Failed:\n{\n    \"error\": {\n        \"code\": \"UserError\",\n        \"message\": \"User program failed with KeyError: 'Attrition_Yes'\",\n        \"messageParameters\": {},\n        \"detailsUri\": \"https://aka.ms/azureml-run-troubleshooting\",\n        \"details\": []\n    },\n    \"time\": \"0001-01-01T00:00:00.000Z\"\n}\n\tInnerException None\n\tErrorResponse \n{\n    \"error\": {\n        \"message\": \"Activity Failed:\\n{\\n    \\\"error\\\": {\\n        \\\"code\\\": \\\"UserError\\\",\\n        \\\"message\\\": \\\"User program failed with KeyError: 'Attrition_Yes'\\\",\\n        \\\"messageParameters\\\": {},\\n        \\\"detailsUri\\\": \\\"https://aka.ms/azureml-run-troubleshooting\\\",\\n        \\\"details\\\": []\\n    },\\n    \\\"time\\\": \\\"0001-01-01T00:00:00.000Z\\\"\\n}\"\n    }\n}"
     ]
    }
   ],
   "source": [
    "from azureml.core import Experiment\n",
    "from azureml.pipeline.core import Pipeline\n",
    "from azureml.widgets import RunDetails\n",
    "\n",
    "# Construct the pipeline\n",
    "pipeline_steps = [train_step, register_step]\n",
    "pipeline = Pipeline(workspace=ws, steps=pipeline_steps)\n",
    "print(\"Pipeline is built.\")\n",
    "\n",
    "# Create an experiment and run the pipeline\n",
    "experiment = Experiment(workspace=ws, name = 'churn-pipeline')\n",
    "pipeline_run = experiment.submit(pipeline, regenerate_outputs=True)\n",
    "print(\"Pipeline submitted for execution.\")\n",
    "RunDetails(pipeline_run).show()\n",
    "pipeline_run.wait_for_completion(show_output=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
